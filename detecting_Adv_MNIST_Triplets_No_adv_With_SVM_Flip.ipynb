{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\ProgramData\\anaconda3\\envs\\spd\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\ProgramData\\anaconda3\\envs\\spd\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\ProgramData\\anaconda3\\envs\\spd\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\ProgramData\\anaconda3\\envs\\spd\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\ProgramData\\anaconda3\\envs\\spd\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\ProgramData\\anaconda3\\envs\\spd\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Located Cleverhans\n",
      "Located Carlini_nn_robust_attacks\n",
      "Located Keras-deep-learning-models\n",
      "Located MobileNets\n",
      "Located Deepfool/Universal\n",
      "Located DenseNet\n",
      "Located MagNet\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from  svhn import SVHNDataset\n",
    "from datasets_utils import *\n",
    "from mnist import MNISTDataset\n",
    "from cifar10 import CIFAR10Dataset\n",
    "from imagenet import ImageNetDataset\n",
    "import itertools\n",
    "\n",
    "from datasets_utils import get_correct_prediction_idx, evaluate_adversarial_examples, calculate_mean_confidence, calculate_accuracy\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import app\n",
    "from tensorflow.python.platform import flags\n",
    "from MNIST_Util_filt_clahe import filtering as medfilter\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_string('dataset_name', 'MNIST', 'Supported: MNIST, CIFAR-10, ImageNet, SVHN.')\n",
    "flags.DEFINE_string('model_name', 'cleverhans', 'Supported: cleverhans, cleverhans_adv_trained and carlini for MNIST; carlini and DenseNet for CIFAR-10;  ResNet50, VGG19, Inceptionv3 and MobileNet for ImageNet; tohinz for SVHN.')\n",
    "flags.DEFINE_boolean('select', True, 'Select correctly classified examples for the experiement.')\n",
    "flags.DEFINE_boolean('balance_sampling', False, 'Select the same number of examples for each class.')\n",
    "flags.DEFINE_boolean('test_mode', False, 'Only select one sample for each class.')\n",
    "flags.DEFINE_integer('nb_examples', 100, 'The number of examples selected for attacks.')\n",
    "flags.DEFINE_string('result_folder', \"results\", 'The output folder for results.')\n",
    "flags.DEFINE_string('attacks',\"FGSM?eps=0.1;BIM?eps=0.1&eps_iter=0.02;JSMA?targeted=next;CarliniL2?targeted=next&batch_size=100&max_iterations=1000;CarliniL2?targeted=next&batch_size=100&max_iterations=1000&confidence=2\", 'Attack name and parameters in URL style, separated by semicolon.')\n",
    "flags.DEFINE_float('clip', -1, 'L-infinity clip on the adversarial perturbations.')\n",
    "flags.DEFINE_boolean('visualize', True, 'Output the image examples for each attack, enabled by default.')\n",
    "flags.DEFINE_boolean('verbose', False, 'Stdout level. The hidden content will be saved to log files anyway.')\n",
    "# flags.DEFINE_string('detection', '', 'Supported: feature_squeezing.')\n",
    "flags.DEFINE_string('detection', \"FeatureSqueezing?squeezers=bit_depth_1&distance_measure=l1&fpr=0.05;FeatureSqueezing?squeezers=bit_depth_2&distance_measure=l1&fpr=0.05;FeatureSqueezing?squeezers=bit_depth_1,median_filter_2_2&distance_measure=l1&fpr=0.05;\", 'Supported: feature_squeezing.')\n",
    "flags.DEFINE_boolean('detection_train_test_mode', False, 'Split into train/test datasets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS.model_name     = 'cleverhans'\n",
    "FLAGS.dataset_name     = 'MNIST'\n",
    "FLAGS.model_name       = 'carlini'\n",
    "FLAGS.select           = True\n",
    "FLAGS.balance_sampling = True\n",
    "FLAGS.test_mode        = False\n",
    "FLAGS.nb_examples      = 100\n",
    "FLAGS.result_folder    = \"results\"\n",
    "# FLAGS.attacks          =\"carlinili?targeted=next&batch_size=100&max_iterations=1000&confidence=10;\"\n",
    "FLAGS.attacks          =\"\\\n",
    "fgsm?eps=0.3;\\\n",
    "bim?eps=0.3&eps_iter=0.06;\\\n",
    "deepfool?overshoot=10;\\\n",
    "carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10;\\\n",
    "carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10;\\\n",
    "carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10;\\\n",
    "carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10;\\\n",
    "carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10;\\\n",
    "carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10;\\\n",
    "jsma?targeted=next;\\\n",
    "jsma?targeted=ll\" \n",
    "# FLAGS.attacks = \"fgsm?eps=0.3;deepfool?overshoot=10\"\n",
    "# FLAGS.attacks = \"jsma?targeted=next\"\n",
    "# FLAGS.detection = \\\n",
    "# \"FeatureSqueezing?squeezers=bit_depth_1&distance_measure=l1&fpr=0.05;\\\n",
    "# FeatureSqueezing?squeezers=bit_depth_2&distance_measure=l1&fpr=0.05;\\\n",
    "# FeatureSqueezing?squeezers=bit_depth_1,median_filter_2_2&distance_measure=l1&fpr=0.05;\"\n",
    "\n",
    "FLAGS.detection =\"FeatureSqueezing?squeezers=bit_depth_1,median_filter_2_2&distance_measure=l1&fpr=0.05;\"\n",
    "\n",
    "# FLAGS.attacks         = \"deepfool?overshoot=10;\"\n",
    "# FLAGS.clip            = -1\n",
    "FLAGS.visualize        = True\n",
    "FLAGS.verbose          = False\n",
    "\n",
    "def load_tf_session():\n",
    "    # Set TF random seed to improve reproducibility\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "    # Create TF session and set as Keras backend session\n",
    "    sess = tf.Session()\n",
    "    keras.backend.set_session(sess)\n",
    "    print(\"Created TensorFlow session and set Keras backend.\")\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Loading MNIST data...\n",
      "Created TensorFlow session and set Keras backend.\n",
      "\n",
      "===Defined TensorFlow model graph.\n",
      "---Loaded MNIST-carlini model.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               205000    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 312,202\n",
      "Trainable params: 312,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defining the dataset\n",
    "if FLAGS.dataset_name == \"MNIST\":\n",
    "    dataset = MNISTDataset()\n",
    "\n",
    "# 1. Load a dataset.\n",
    "print(\"\\n===Loading %s data...\" % FLAGS.dataset_name)\n",
    "\n",
    "if FLAGS.dataset_name == 'ImageNet':\n",
    "    if FLAGS.model_name == 'inceptionv3':\n",
    "        img_size = 299\n",
    "    else:\n",
    "        img_size = 224\n",
    "    X_test_all, Y_test_all = dataset.get_test_data(img_size, 0, 200)\n",
    "else:\n",
    "    X_test_all, Y_test_all = dataset.get_test_dataset()\n",
    "\n",
    "# 2. Load a trained model.\n",
    "\n",
    "sess = load_tf_session()\n",
    "keras.backend.set_learning_phase(0)\n",
    "# Define input TF placeholder\n",
    "x = tf.placeholder(tf.float32, shape=(None, dataset.image_size, dataset.image_size, dataset.num_channels))\n",
    "y = tf.placeholder(tf.float32, shape=(None, dataset.num_classes))\n",
    "\n",
    "with tf.variable_scope(FLAGS.model_name):\n",
    "    \"\"\"\n",
    "    Create a model instance for prediction.\n",
    "    The scaling argument, 'input_range_type': {1: [0,1], 2:[-0.5, 0.5], 3:[-1, 1]...}\n",
    "    \"\"\"\n",
    "    model = dataset.load_model_by_name(FLAGS.model_name, logits=False, input_range_type=1)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['acc'])\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the pre-trained model...\n",
      "Test accuracy on raw legitimate examples 0.9943\n",
      "Mean confidence on ground truth classes 0.9939\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluate the trained model.\n",
    "print (\"Evaluating the pre-trained model...\")\n",
    "Y_pred_all = model.predict(X_test_all)\n",
    "mean_conf_all = calculate_mean_confidence(Y_pred_all, Y_test_all)\n",
    "accuracy_all = calculate_accuracy(Y_pred_all, Y_test_all)\n",
    "print('Test accuracy on raw legitimate examples %.4f' % (accuracy_all))\n",
    "print('Mean confidence on ground truth classes %.4f' % (mean_conf_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 examples.\n",
      "Selected index in test set (sorted): 0-64:1,66,68-73:1,75-78:1,81,82,84,87,88,90-92:1,98,101,102-110:4,119,120,126-128:1,134,146,177-181:2,184\n",
      "Test accuracy on selected legitimate examples 1.0000\n",
      "Mean confidence on ground truth classes, selected 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Select some examples to attack.\n",
    "import hashlib\n",
    "from datasets_utils import get_first_n_examples_id_each_class\n",
    "if FLAGS.select:\n",
    "    # Filter out the misclassified examples.\n",
    "    correct_idx = get_correct_prediction_idx(Y_pred_all, Y_test_all)\n",
    "    if FLAGS.test_mode:\n",
    "        # Only select the first example of each class.\n",
    "        correct_and_selected_idx = get_first_n_examples_id_each_class(Y_test_all[correct_idx])\n",
    "        selected_idx = [ correct_idx[i] for i in correct_and_selected_idx ]\n",
    "    else:\n",
    "        if not FLAGS.balance_sampling:\n",
    "            selected_idx = correct_idx[:FLAGS.nb_examples]\n",
    "        else:\n",
    "            # select the same number of examples for each class label.\n",
    "            nb_examples_per_class = int(FLAGS.nb_examples / Y_test_all.shape[1])\n",
    "            correct_and_selected_idx = get_first_n_examples_id_each_class(Y_test_all[correct_idx], n=nb_examples_per_class)\n",
    "            selected_idx = [ correct_idx[i] for i in correct_and_selected_idx ]\n",
    "else:\n",
    "    selected_idx = np.array(range(FLAGS.nb_examples))\n",
    "# select the same number of examples for each class label.\n",
    "nb_examples_per_class = int(FLAGS.nb_examples / Y_test_all.shape[1])\n",
    "correct_and_selected_idx = get_first_n_examples_id_each_class(Y_test_all[correct_idx], n=nb_examples_per_class)\n",
    "selected_idx = [ correct_idx[i] for i in correct_and_selected_idx ]\n",
    "\n",
    "from utils.output import format_number_range\n",
    "selected_example_idx_ranges = format_number_range(sorted(selected_idx))\n",
    "print ( \"Selected %d examples.\" % len(selected_idx))\n",
    "print ( \"Selected index in test set (sorted): %s\" % selected_example_idx_ranges )\n",
    "X_test, Y_test, Y_pred = X_test_all[selected_idx], Y_test_all[selected_idx], Y_pred_all[selected_idx]\n",
    "\n",
    "# The accuracy should be 100%.\n",
    "accuracy_selected = calculate_accuracy(Y_pred, Y_test)\n",
    "mean_conf_selected = calculate_mean_confidence(Y_pred, Y_test)\n",
    "print('Test accuracy on selected legitimate examples %.4f' % (accuracy_selected))\n",
    "print('Mean confidence on ground truth classes, selected %.4f\\n' % (mean_conf_selected))\n",
    "\n",
    "task = {}\n",
    "task['dataset_name'] = FLAGS.dataset_name\n",
    "task['model_name'] = FLAGS.model_name\n",
    "task['accuracy_test'] = accuracy_all\n",
    "task['mean_confidence_test'] = mean_conf_all\n",
    "\n",
    "task['test_set_selected_length'] = len(selected_idx)\n",
    "task['test_set_selected_idx_ranges'] = selected_example_idx_ranges\n",
    "task['test_set_selected_idx_hash'] = hashlib.sha1(str(selected_idx).encode('utf-8')).hexdigest()\n",
    "task['accuracy_test_selected'] = accuracy_selected\n",
    "task['mean_confidence_test_selected'] = mean_conf_selected\n",
    "\n",
    "task_id = \"%s_%d_%s_%s\" % \\\n",
    "        (task['dataset_name'], task['test_set_selected_length'], task['test_set_selected_idx_hash'][:5], task['model_name'])\n",
    "\n",
    "FLAGS.result_folder = os.path.join(FLAGS.result_folder, task_id)\n",
    "if not os.path.isdir(FLAGS.result_folder):\n",
    "    os.makedirs(FLAGS.result_folder)\n",
    "    \n",
    "from utils.output import save_task_descriptor\n",
    "save_task_descriptor(FLAGS.result_folder, [task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Generate adversarial examples.\n",
    "from attacks import maybe_generate_adv_examples\n",
    "from utils.squeeze import reduce_precision_py\n",
    "from utils.parameter_parser import parse_params\n",
    "attack_string_hash = hashlib.sha1(FLAGS.attacks.encode('utf-8')).hexdigest()[:5]\n",
    "sample_string_hash = task['test_set_selected_idx_hash'][:5]\n",
    "\n",
    "from datasets_utils import get_next_class, get_least_likely_class\n",
    "Y_test_target_next = get_next_class(Y_test)\n",
    "Y_test_target_ll = get_least_likely_class(Y_pred)\n",
    "\n",
    "X_test_adv_list = []\n",
    "X_test_adv_discretized_list = []\n",
    "Y_test_adv_discretized_pred_list = []\n",
    "\n",
    "attack_string_list = filter(lambda x:len(x)>0, FLAGS.attacks.lower().split(';'))\n",
    "to_csv = []\n",
    "\n",
    "X_adv_cache_folder = os.path.join(FLAGS.result_folder, 'adv_examples')\n",
    "adv_log_folder = os.path.join(FLAGS.result_folder, 'adv_logs')\n",
    "predictions_folder = os.path.join(FLAGS.result_folder, 'predictions')\n",
    "for folder in [X_adv_cache_folder, adv_log_folder, predictions_folder]:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "predictions_fpath = os.path.join(predictions_folder, \"legitimate.npy\")\n",
    "np.save(predictions_fpath, Y_pred, allow_pickle=False)\n",
    "\n",
    "if FLAGS.clip >= 0:\n",
    "    epsilon = FLAGS.clip\n",
    "    print (\"Clip the adversarial perturbations by +-%f\" % epsilon)\n",
    "    max_clip = np.clip(X_test + epsilon, 0, 1)\n",
    "    min_clip = np.clip(X_test - epsilon, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running attack: fgsm {'eps': 0.3}\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_fgsm_eps=0.3.pickle].\n",
      "\n",
      "---Attack (uint8): fgsm?eps=0.3\n",
      "Success rate: 46.00%, Mean confidence of SAEs: 94.76%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 5.9158, Li dist: 0.3020, L0 dist_value: 56.1%, L0 dist_pixel: 56.1%\n",
      "\n",
      "Running attack: bim {'eps': 0.3, 'eps_iter': 0.06}\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_bim_eps=0.3&eps_iter=0.06.pickle].\n",
      "\n",
      "---Attack (uint8): bim?eps=0.3&eps_iter=0.06\n",
      "Success rate: 92.00%, Mean confidence of SAEs: 99.82%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 4.8198, Li dist: 0.3020, L0 dist_value: 52.3%, L0 dist_pixel: 52.3%\n",
      "\n",
      "Running attack: deepfool {'overshoot': 10.0}\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_deepfool_overshoot=10.pickle].\n",
      "\n",
      "---Attack (uint8): deepfool?overshoot=10\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 89.16%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 2.1745, Li dist: 0.5322, L0 dist_value: 73.2%, L0 dist_pixel: 73.2%\n",
      "\n",
      "Running attack: carlinili {'targeted': 'next', 'batch_size': 1, 'max_iterations': 1000, 'confidence': 10.0}\n",
      "targeted value: next\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_carlinili_targeted=next&batch_size=1&max_iterations=1000&confidence=10.pickle].\n",
      "\n",
      "---Attack (uint8): carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 99.99%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 4.2770, Li dist: 0.2608, L0 dist_value: 49.6%, L0 dist_pixel: 49.6%\n",
      "\n",
      "Running attack: carlinili {'targeted': 'll', 'batch_size': 1, 'max_iterations': 1000, 'confidence': 10.0}\n",
      "targeted value: ll\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_carlinili_targeted=ll&batch_size=1&max_iterations=1000&confidence=10.pickle].\n",
      "\n",
      "---Attack (uint8): carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 99.98%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 4.6661, Li dist: 0.2792, L0 dist_value: 51.0%, L0 dist_pixel: 51.0%\n",
      "\n",
      "Running attack: carlinil2 {'targeted': 'next', 'batch_size': 100, 'max_iterations': 1000, 'confidence': 10.0}\n",
      "targeted value: next\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_carlinil2_targeted=next&batch_size=100&max_iterations=1000&confidence=10.pickle].\n",
      "\n",
      "---Attack (uint8): carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10\n",
      "Success rate: 99.00%, Mean confidence of SAEs: 99.99%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 2.8780, Li dist: 0.6888, L0 dist_value: 45.8%, L0 dist_pixel: 45.8%\n",
      "\n",
      "Running attack: carlinil2 {'targeted': 'll', 'batch_size': 100, 'max_iterations': 1000, 'confidence': 10.0}\n",
      "targeted value: ll\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_carlinil2_targeted=ll&batch_size=100&max_iterations=1000&confidence=10.pickle].\n",
      "\n",
      "---Attack (uint8): carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 99.99%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 3.2094, Li dist: 0.7326, L0 dist_value: 45.8%, L0 dist_pixel: 45.8%\n",
      "\n",
      "Running attack: carlinil0 {'targeted': 'next', 'batch_size': 1, 'max_iterations': 1000, 'confidence': 10.0}\n",
      "targeted value: next\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_carlinil0_targeted=next&batch_size=1&max_iterations=1000&confidence=10.pickle].\n",
      "\n",
      "---Attack (uint8): carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 99.99%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 4.7365, Li dist: 0.9948, L0 dist_value: 5.1%, L0 dist_pixel: 5.1%\n",
      "\n",
      "Running attack: carlinil0 {'targeted': 'll', 'batch_size': 1, 'max_iterations': 1000, 'confidence': 10.0}\n",
      "targeted value: ll\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_carlinil0_targeted=ll&batch_size=1&max_iterations=1000&confidence=10.pickle].\n",
      "\n",
      "---Attack (uint8): carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 99.99%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 5.1723, Li dist: 0.9971, L0 dist_value: 6.0%, L0 dist_pixel: 6.0%\n",
      "\n",
      "Running attack: jsma {'targeted': 'next'}\n",
      "targeted value: next\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_jsma_targeted=next.pickle].\n",
      "\n",
      "---Attack (uint8): jsma?targeted=next\n",
      "Success rate: 63.00%, Mean confidence of SAEs: 64.92%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 4.8140, Li dist: 1.0000, L0 dist_value: 4.9%, L0 dist_pixel: 4.9%\n",
      "\n",
      "Running attack: jsma {'targeted': 'll'}\n",
      "targeted value: ll\n",
      "Loading adversarial examples from [MNIST_100_2cd66_carlini_jsma_targeted=ll.pickle].\n",
      "\n",
      "---Attack (uint8): jsma?targeted=ll\n",
      "Success rate: 45.00%, Mean confidence of SAEs: 64.54%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 5.6190, Li dist: 1.0000, L0 dist_value: 6.4%, L0 dist_pixel: 6.4%\n"
     ]
    }
   ],
   "source": [
    "for attack_string in attack_string_list:\n",
    "    attack_log_fpath = os.path.join(adv_log_folder, \"%s_%s.log\" % (task_id, attack_string.replace(\"?\",\"_\")))\n",
    "    attack_name, attack_params = parse_params(attack_string)\n",
    "    print ( \"\\nRunning attack: %s %s\" % (attack_name, attack_params))\n",
    "\n",
    "    if 'targeted' in attack_params:\n",
    "        targeted = attack_params['targeted']\n",
    "        print (\"targeted value: %s\" % targeted)\n",
    "        if targeted == 'next':\n",
    "            Y_test_target = Y_test_target_next\n",
    "        elif targeted == 'll':\n",
    "            Y_test_target = Y_test_target_ll\n",
    "        elif targeted == False:\n",
    "            attack_params['targeted'] = False\n",
    "            Y_test_target = Y_test.copy()\n",
    "    else:\n",
    "        targeted = False\n",
    "        attack_params['targeted'] = False\n",
    "        Y_test_target = Y_test.copy()\n",
    "    # num_classes = Y_test_all[0].shape[0] #only for Deep Fool\n",
    "    # attack_params['num_classes'] = num_classes\n",
    "\n",
    "    x_adv_fname = \"%s_%s.pickle\" % (task_id, attack_string.replace(\"?\",\"_\"))\n",
    "    x_adv_fpath = os.path.join(X_adv_cache_folder, x_adv_fname)\n",
    "\n",
    "    X_test_adv, aux_info = maybe_generate_adv_examples(sess, model, x, y, X_test, Y_test_target, attack_name, attack_params,\n",
    "                                                        use_cache = x_adv_fpath, verbose=FLAGS.verbose, attack_log_fpath=attack_log_fpath)\n",
    "\n",
    "    if FLAGS.clip > 0:\n",
    "        # This is L-inf clipping.\n",
    "        X_test_adv = np.clip(X_test_adv, min_clip, max_clip)\n",
    "\n",
    "    X_test_adv_list.append(X_test_adv)\n",
    "\n",
    "    if isinstance(aux_info, float):\n",
    "        duration = aux_info\n",
    "    else:\n",
    "        duration = aux_info['duration']\n",
    "\n",
    "    dur_per_sample = duration / len(X_test_adv)\n",
    "    # 5.0 Output predictions.\n",
    "    Y_test_adv_pred = model.predict(X_test_adv)\n",
    "    predictions_fpath = os.path.join(predictions_folder, \"%s.npy\"% attack_string.replace(\"?\",\"_\"))\n",
    "    np.save(predictions_fpath, Y_test_adv_pred, allow_pickle=False)\n",
    "\n",
    "    # 5.1 Evaluate the adversarial examples being discretized to uint8.\n",
    "    print (\"\\n---Attack (uint8): %s\" % attack_string)\n",
    "    # All data should be discretized to uint8.\n",
    "    X_test_adv_discret = reduce_precision_py(X_test_adv, 256)\n",
    "    X_test_adv_discretized_list.append(X_test_adv_discret)\n",
    "    Y_test_adv_discret_pred = model.predict(X_test_adv_discret)\n",
    "    Y_test_adv_discretized_pred_list.append(Y_test_adv_discret_pred)\n",
    "\n",
    "    rec = evaluate_adversarial_examples(X_test, Y_test, X_test_adv_discret, Y_test_target.copy(), targeted, Y_test_adv_discret_pred)\n",
    "    rec['dataset_name'] = FLAGS.dataset_name\n",
    "    rec['model_name'] = FLAGS.model_name\n",
    "    rec['attack_string'] = attack_string.replace(\"?\",\"_\")\n",
    "    rec['duration_per_sample'] = dur_per_sample\n",
    "    rec['discretization'] = True\n",
    "    to_csv.append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.output import write_to_csv\n",
    "# attacks_evaluation_csv_fpath = os.path.join(FLAGS.result_folder, \n",
    "#         \"%s_attacks_%s_evaluation.csv\" % \\\n",
    "#         (task_id, attack_string_hash))\n",
    "# fieldnames = ['dataset_name', 'model_name', 'attack_string', 'duration_per_sample', 'discretization',\n",
    "#                'success_rate', 'mean_confidence', 'mean_l2_dist', 'mean_li_dist', 'mean_l0_dist_value', 'mean_l0_dist_pixel']\n",
    "# write_to_csv(to_csv, attacks_evaluation_csv_fpath, fieldnames)\n",
    "\n",
    "\n",
    "# if FLAGS.visualize is True:\n",
    "#     from visualization import show_imgs_in_rows\n",
    "\n",
    "#     selected_idx_vis = get_first_n_examples_id_each_class(Y_test, 1)\n",
    "\n",
    "#     legitimate_examples = X_test[selected_idx_vis]\n",
    "\n",
    "#     rows = [legitimate_examples]\n",
    "#     rows += map(lambda x:x[selected_idx_vis], X_test_adv_list)\n",
    "\n",
    "#     img_fpath = os.path.join(FLAGS.result_folder, '%s_attacks_%s_examples.png' % (task_id, attack_string_hash) )\n",
    "#     show_imgs_in_rows(rows, img_fpath)\n",
    "#     print ('\\n===Adversarial image examples are saved in ', img_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from feature_squeezing import FeatureSqueezingDetector\n",
    "from magnet_mnist import MagNetDetector as MagNetDetectorMNIST\n",
    "from magnet_cifar import MagNetDetector as MagNetDetectorCIFAR\n",
    "\n",
    "from tensorflow.python.platform import flags\n",
    "FLAGS = flags.FLAGS\n",
    "from utils.output import write_to_csv\n",
    "from Segmentation_detector import SegmentationDetector \n",
    "\n",
    "def get_tpr_fpr(true_labels, pred_labels):\n",
    "    TP = np.sum(np.logical_and(pred_labels == 1, true_labels == 1))\n",
    "    FP = np.sum(np.logical_and(pred_labels == 1, true_labels == 0))\n",
    "\n",
    "    AP = np.sum(true_labels)\n",
    "    AN = np.sum(1-true_labels)\n",
    "\n",
    "    tpr = TP/AP if AP>0 else np.nan\n",
    "    fpr = FP/AN if AN>0 else np.nan\n",
    "\n",
    "    return tpr, fpr, TP, AP\n",
    "\n",
    "\n",
    "def evalulate_detection_test(Y_detect_test, Y_detect_pred):\n",
    "    accuracy = sklearn.metrics.accuracy_score(Y_detect_test, Y_detect_pred, normalize=True, sample_weight=None)\n",
    "    tpr, fpr, tp, ap = get_tpr_fpr(Y_detect_test, Y_detect_pred)\n",
    "    return accuracy, tpr, fpr, tp, ap\n",
    "\n",
    "\n",
    "from tinydb import TinyDB, Query\n",
    "\n",
    "class DetectionEvaluator:\n",
    "    \"\"\"\n",
    "    Get a dataset;\n",
    "        Failed adv as benign / Failed adv as adversarial.\n",
    "    For each detector:\n",
    "        Train\n",
    "        Test\n",
    "        Report performance\n",
    "            Detection rate on each attack.\n",
    "            Detection on SAEs / FAEs.\n",
    "            ROC-AUC.\n",
    "\n",
    "    A detector should have this simplified interface:\n",
    "        Y_pred = detector(X)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, result_folder, csv_fname, dataset_name):\n",
    "        pass\n",
    "        # set_base_model()\n",
    "        self.model = model\n",
    "        self.task_dir = result_folder\n",
    "        self.csv_fpath = os.path.join(result_folder, csv_fname)\n",
    "        print(self.csv_fpath)\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        if not os.path.isdir(self.task_dir):\n",
    "            os.makedirs(self.task_dir)\n",
    "\n",
    "    def get_attack_id(self, attack_name):\n",
    "        return self.attack_name_id[attack_name]\n",
    "\n",
    "    def build_detection_dataset(self, X, Y_label, Y_pred, selected_idx, X_adv_list, Y_adv_pred_list,\n",
    "                                 attack_names, attack_string_hash, clip, Y_test_target_next, Y_test_target_ll):\n",
    "        # X_train, Y_train, X_test, Y_test, test_idx, failed_adv_idx = \\\n",
    "        #     get_detection_train_test_set(X, Y_label, X_adv_list, Y_adv_pred_list, attack_names)\n",
    "\n",
    "        \"\"\"\n",
    "        Data Model:\n",
    "            index, attack_id, misclassified, train\n",
    "            14,     0,           0,             1\n",
    "        \"\"\"\n",
    "\n",
    "        self.attack_names = attack_names.lower().split(';')\n",
    "        self.attack_name_id = {}\n",
    "        self.attack_name_id['legitimate'] = 0\n",
    "        # attack_names = filter(lambda x:len(x)>0, attack_names.lower().split(';'))\n",
    "        attack_names = attack_names.lower().split(';')\n",
    "        for i,attack_name in enumerate(attack_names):\n",
    "            # print(\"AttackNames\",i,attack_name)\n",
    "            self.attack_name_id[attack_name] = i+1\n",
    "            # print(self.attack_name_id[attack_name])\n",
    "\n",
    "        X_adv_all = np.concatenate(X_adv_list)\n",
    "        X_leg_all = X[:len(X_adv_all)]\n",
    "        # print(len(X_leg_all),\"kkkkkkk\")\n",
    "        self.X_detect = X_detect = np.concatenate([X_leg_all, X_adv_all])\n",
    "        # TODO: this could be wrong in non-default data selection mode.\n",
    "        Y_label_adv = Y_label[selected_idx]\n",
    "\n",
    "        detection_db_path = os.path.join(self.task_dir, \"detection_db_%s_clip_%s.json\" % (attack_string_hash, clip))\n",
    "\n",
    "        if os.path.isfile(detection_db_path):\n",
    "            self.db = TinyDB(detection_db_path)\n",
    "            self.query = Query()\n",
    "            print (\"Loaded an existing detection dataset.\")\n",
    "            return\n",
    "        else:\n",
    "            print (\"Preparing the detection dataset...\")\n",
    "\n",
    "        # 1. Split Train and Test \n",
    "        random.seed(1234)\n",
    "        length = len(X_detect)\n",
    "        train_ratio = 0.5\n",
    "        train_idx = random.sample(range(length), int(train_ratio*length))\n",
    "        train_test_seq = [1 if idx in train_idx else 0 for idx in range(length) ]\n",
    "\n",
    "        # 2. Tag the misclassified examples, both legitimate and adversarial.\n",
    "        # TODO: Differentiate the successful examples between targeted and non-targeted.\n",
    "        misclassified_seq = list(np.argmax(Y_label[:len(X_leg_all)], axis=1) != np.argmax(Y_pred[:len(X_leg_all)], axis=1))\n",
    "        for Y_adv_pred in Y_adv_pred_list:\n",
    "            misclassified_seq_adv = list(np.argmax(Y_adv_pred, axis=1) != np.argmax(Y_label_adv, axis=1))\n",
    "            misclassified_seq += misclassified_seq_adv\n",
    "\n",
    "        success_adv_seq = [False] * len(X_leg_all)\n",
    "        for ( Y_adv_pred),(attack_name_i) in zip(Y_adv_pred_list,attack_names):\n",
    "            attack_name = attack_name_i\n",
    "            if 'targeted=ll' in attack_name:\n",
    "                # print(\"Yes LL\")\n",
    "                success_adv_seq_attack = list(np.argmax(Y_adv_pred, axis=1) == np.argmax(Y_test_target_ll, axis=1))\n",
    "            elif 'targeted=next' in attack_name:\n",
    "                # print(\"Yes Next\")\n",
    "                success_adv_seq_attack = list(np.argmax(Y_adv_pred, axis=1) == np.argmax(Y_test_target_next, axis=1))\n",
    "            else:\n",
    "                # The same as misclassified.\n",
    "                success_adv_seq_attack = list(np.argmax(Y_adv_pred, axis=1) != np.argmax(Y_label_adv, axis=1))\n",
    "            success_adv_seq += success_adv_seq_attack\n",
    "\n",
    "\n",
    "\n",
    "        # 3. Tag the attack ID, 0 as legitimate.\n",
    "        attack_id_seq = [0]*len(X_leg_all)\n",
    "        # attack_id_seq = [0]*len(X_detect)\n",
    "        # print(\"Hello_Hi\")\n",
    "        # print(len(attack_names),len(attack_id_seq))\n",
    "        for i,attack_name in enumerate(attack_names):\n",
    "            attack_id_seq += [i+1]*len(X_adv_list[0])\n",
    "            # print(\"Hello_friend\")\n",
    "        # print(len(X_detect) , len(train_test_seq) , len(misclassified_seq) , len(attack_id_seq))\n",
    "        assert len(X_detect) == len(train_test_seq) == len(misclassified_seq) == len(attack_id_seq)\n",
    "        # assert len(X_detect) == len(train_test_seq) == len(misclassified_seq)\n",
    "\n",
    "        self.db = TinyDB(detection_db_path)\n",
    "        self.query = Query()\n",
    "        # print(len(attack_id_seq),len(X_detect))\n",
    "        for i in range(len(X_detect)):\n",
    "            attack_id = attack_id_seq[i]\n",
    "            misclassified = 1 if misclassified_seq[i] == True else 0\n",
    "            success = 1 if success_adv_seq[i] == True else 0\n",
    "            train = train_test_seq[i]\n",
    "            rec = {'index': i, 'attack_id': attack_id, 'misclassified': misclassified, 'success': success, 'train': train}\n",
    "            self.db.insert(rec)\n",
    "\n",
    "    def get_data_from_db_records(self, recs):\n",
    "        if len(recs) == 0:\n",
    "            return None, None\n",
    "        X_idx = [rec['index'] for rec in recs]\n",
    "        X = self.X_detect[np.array(X_idx)]\n",
    "        Y = np.array([1 if rec['attack_id']>0 else 0 for rec in recs])\n",
    "        return X, Y\n",
    "\n",
    "    def get_training_testing_data(self, train = True):\n",
    "        db = self.db\n",
    "        query = self.query\n",
    "\n",
    "        recs = db.search(query.train == 1)\n",
    "        X_train, Y_train = self.get_data_from_db_records(recs)\n",
    "\n",
    "        recs = db.search(query.train == 0)\n",
    "        X_test, Y_test = self.get_data_from_db_records(recs)\n",
    "\n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "    def get_adversarial_data(self, only_testing, success, attack_name=None, include_legitimate=False):\n",
    "        db = self.db\n",
    "        query = self.query\n",
    "\n",
    "        conditions_and = []\n",
    "        if only_testing:\n",
    "            conditions_and.append(query.train == 0)\n",
    "\n",
    "        if attack_name is None:\n",
    "            conditions_and.append(query.attack_id > 0)\n",
    "            # print(query.attack_id,\"1\")\n",
    "        else:\n",
    "            # print(attack_name,\"2\")\n",
    "            attack_id = self.get_attack_id(attack_name)\n",
    "            conditions_and.append(query.attack_id == attack_id)\n",
    "\n",
    "        if success:\n",
    "            conditions_and.append(query.success == 1)\n",
    "        else:\n",
    "            conditions_and.append(query.success == 0)\n",
    "\n",
    "        conditions = reduce(lambda a,b:a&b, conditions_and)\n",
    "        print (\"conditions: %s \" % conditions)\n",
    "\n",
    "        recs = db.search(conditions)\n",
    "\n",
    "        if include_legitimate:\n",
    "            if only_testing:\n",
    "                conditions = (query.attack_id == 0) & (query.train == 0)\n",
    "            else:\n",
    "                conditions = query.attack_id == 0\n",
    "            # print (\"additional conditions: %s \" % conditions)\n",
    "            recs += db.search(conditions)\n",
    "\n",
    "        return self.get_data_from_db_records(recs)\n",
    "\n",
    "    def get_sae_testing_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=True, success=True, attack_name=attack_name)\n",
    "\n",
    "    def get_sae_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=False, success=True, attack_name=attack_name)\n",
    "\n",
    "    def get_fae_testing_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=True, success=False, attack_name=attack_name)\n",
    "\n",
    "    def get_fae_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=False, success=False, attack_name=attack_name)\n",
    "\n",
    "    def get_all_non_fae_testing_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=True, success=True, attack_name=attack_name, include_legitimate=True)\n",
    "\n",
    "    def get_all_non_fae_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=False, success=True, attack_name=attack_name, include_legitimate=True)\n",
    "\n",
    "    def get_detector_by_name(self, detector_name):\n",
    "        model = self.model\n",
    "        detector = None\n",
    "\n",
    "        if detector_name.startswith('FeatureSqueezing'):\n",
    "            detector = FeatureSqueezingDetector(model, detector_name)\n",
    "        elif detector_name.startswith('Segmentation'):\n",
    "            detector = SegmentationDetector()\n",
    "        elif detector_name.startswith('MagNet'):\n",
    "            if self.dataset_name == 'MNIST':\n",
    "                detector = MagNetDetectorMNIST(model, detector_name)\n",
    "            elif self.dataset_name == \"CIFAR-10\":\n",
    "                detector = MagNetDetectorCIFAR(model, detector_name)\n",
    "\n",
    "        return detector\n",
    "\n",
    "    def evaluate_detections(self, params_str,X_test_all,Y_test_all):\n",
    "        X_train, Y_train, X_test, Y_test = self.get_training_testing_data()\n",
    "        # print(Y_test)\n",
    "        # Example: --detection \"FeatureSqueezing?distance_measure=l1&squeezers=median_smoothing_2,bit_depth_4;\"\n",
    "        detector_names = [ele.strip() for ele in params_str.split(';') if ele.strip()!= '']\n",
    "        dataset_name = self.dataset_name\n",
    "        csv_fpath = \"./segmentation_detection_%s_saes.csv\" % dataset_name \n",
    "        fieldnames = ['detector', 'threshold', 'fpr','fpr_all_test_clean','Accuracy_all_test_clean'] + list(self.attack_names) + ['overall']\n",
    "        print(fieldnames)\n",
    "        to_csv = []\n",
    "\n",
    "        for detector_name in detector_names:\n",
    "            detector = self.get_detector_by_name(detector_name)\n",
    "            if detector is None:\n",
    "                print (\"Skipped an unknown detector [%s]\" % detector_name.split('?')[0])\n",
    "                continue\n",
    "            detector.train(X_train, Y_train)\n",
    "            Y_test_pred, Y_test_pred_score = detector.test(X_test)\n",
    "            # print( \"Hello Boy\" )\n",
    "            accuracy, tpr, fpr, tp, ap = evalulate_detection_test(Y_test, Y_test_pred)\n",
    "            fprs, tprs, thresholds = roc_curve(Y_test, Y_test_pred_score)\n",
    "            roc_auc = auc(fprs, tprs)\n",
    "            print( \"\\n\" )\n",
    "            print (\"Detector: %s\" % detector_name)\n",
    "            print( \"\\n\" )\n",
    "            print (\"Accuracy: %f\\tTPR: %f\\tFPR: %f\\tROC-AUC: %f\" % (accuracy, tpr, fpr, roc_auc))\n",
    "\n",
    "            Y_test_pred_t, Y_test_pred_score_t = detector.test(X_test_all)\n",
    "            # print(Y_test_pred_t[0], \"Hello Boy\" )\n",
    "            Y_test_thresh = np.zeros(np.array(np.argmax(Y_test_all,axis=1)).shape) \n",
    "            accuracy_all, tpr_all, fpr_all, tp_all, ap_all = evalulate_detection_test(Y_test_thresh, Y_test_pred_t)\n",
    "            print(\"Accuracy_all_test:%.3f\" % accuracy_all,\" FPR_all_test:%.3f\" % fpr_all,\"For all TestSet\")\n",
    "            \n",
    "            rec = {}\n",
    "            rec['detector'] = detector_name\n",
    "            if hasattr(detector, 'threshold'):\n",
    "                rec['threshold'] = detector.threshold\n",
    "            else:\n",
    "                rec['threshold'] = None\n",
    "            rec['fpr'] = fpr\n",
    "            rec['Accuracy_all_test_clean'] = accuracy_all\n",
    "            rec['fpr_all_test_clean'] = fpr_all\n",
    "            overall_detection_rate_saes = 0\n",
    "            nb_saes = 0\n",
    "            for attack_name in self.attack_names:\n",
    "                print( \"\\n\" )\n",
    "                # No adversarial examples for training for the current detection methods.\n",
    "                # X_sae, Y_sae = self.get_sae_testing_data(attack_name)\n",
    "                if FLAGS.detection_train_test_mode:\n",
    "                    print(attack_name)\n",
    "                    X_sae, Y_sae = self.get_sae_testing_data(attack_name)\n",
    "                else:\n",
    "                    X_sae, Y_sae = self.get_sae_data(attack_name)\n",
    "                Y_test_pred, Y_test_pred_score = detector.test(X_sae)\n",
    "                # print(Y_sae,Y_test_pred,\"Hala Madrid\")\n",
    "                _, tpr, _, tp, ap = evalulate_detection_test(Y_sae, Y_test_pred)\n",
    "                print (\"Detection rate on SAEs: %.4f \\t %3d/%3d \\t %s\" % (tpr, tp, ap, attack_name))\n",
    "                overall_detection_rate_saes += tpr * len(Y_sae)\n",
    "                nb_saes += len(Y_sae)\n",
    "                rec[attack_name] = tpr\n",
    "                # print (\"overall_detection_rate_saes/nb_saes: %d/%d\" % (overall_detection_rate_saes, nb_saes))\n",
    "            # print(len(nb_saes))\n",
    "            print(nb_saes)\n",
    "            print (\"Overall detection rate on SAEs: %f (%d/%d)\" % (overall_detection_rate_saes/nb_saes, overall_detection_rate_saes, nb_saes))\n",
    "            rec['overall'] = float(overall_detection_rate_saes/nb_saes)\n",
    "            to_csv.append(rec)\n",
    "\n",
    "            # No adversarial examples for training for the current detection methods.\n",
    "            # X_sae_all, Y_sae_all = self.get_sae_testing_data()\n",
    "            print (\"### Excluding FAEs:\")\n",
    "            if FLAGS.detection_train_test_mode:\n",
    "                X_nfae_all, Y_nfae_all = self.get_all_non_fae_testing_data()\n",
    "            else:\n",
    "                X_nfae_all, Y_nfae_all = self.get_all_non_fae_data()\n",
    "            Y_pred, Y_pred_score = detector.test(X_nfae_all)\n",
    "            _, tpr, _, tp, ap = evalulate_detection_test(Y_nfae_all, Y_pred)\n",
    "            fprs, tprs, thresholds = roc_curve(Y_nfae_all, Y_pred_score)\n",
    "\n",
    "            # print (\"threshold\\tfpr\\ttpr\")\n",
    "            # for i, threshold  in enumerate(thresholds):\n",
    "            #     print (\"%.4f\\t%.4f\\t%.4f\" % (threshold, fprs[i], tprs[i]))\n",
    "\n",
    "            roc_auc = auc(fprs, tprs)\n",
    "            print (\"Overall TPR: %f\\tROC-AUC: %f\" % (tpr, roc_auc))\n",
    "\n",
    "            # FAEs\n",
    "            if FLAGS.detection_train_test_mode:\n",
    "                X_fae, Y_fae = self.get_fae_testing_data()\n",
    "            else:\n",
    "                X_fae, Y_fae = self.get_fae_data()\n",
    "            Y_test_pred, Y_test_pred_score = detector.test(X_fae)\n",
    "            _, tpr, _, tp, ap = evalulate_detection_test(Y_fae, Y_test_pred)\n",
    "            print (\"Overall detection rate on FAEs: %.4f \\t %3d/%3d\" % (tpr, tp, ap))\n",
    "        # print(csv_fpath,fieldnames)\n",
    "        write_to_csv(to_csv, csv_fpath, fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\\MNIST_100_2cd66_carlini\\detection\\MNIST_100_2cd66_carlini_attacks_f64f6_detection.csv\n",
      "Preparing the detection dataset...\n",
      "['detector', 'threshold', 'fpr', 'fpr_all_test_clean', 'Accuracy_all_test_clean', 'fgsm?eps=0.3', 'bim?eps=0.3&eps_iter=0.06', 'deepfool?overshoot=10', 'carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10', 'carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10', 'carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10', 'carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10', 'carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10', 'carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10', 'jsma?targeted=next', 'jsma?targeted=ll', 'overall']\n",
      "Selected 0.000252 as the threshold value.\n",
      "\n",
      "\n",
      "Detector: FeatureSqueezing?squeezers=bit_depth_1&distance_measure=l1&fpr=0.05\n",
      "\n",
      "\n",
      "Accuracy: 0.904545\tTPR: 0.854779\tFPR: 0.046763\tROC-AUC: 0.932807\n",
      "Accuracy_all_test:0.958  FPR_all_test:0.042 For all TestSet\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 1), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.9783 \t  45/ 46 \t fgsm?eps=0.3\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 2)})) \n",
      "Detection rate on SAEs: 0.9891 \t  91/ 92 \t bim?eps=0.3&eps_iter=0.06\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 3), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t deepfool?overshoot=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 4)})) \n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 5)})) \n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 6), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t  99/ 99 \t carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 7), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 8), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.7100 \t  71/100 \t carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 9), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.6400 \t  64/100 \t carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 10)})) \n",
      "Detection rate on SAEs: 1.0000 \t  63/ 63 \t jsma?targeted=next\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 11), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t  45/ 45 \t jsma?targeted=ll\n",
      "945\n",
      "Overall detection rate on SAEs: 0.929101 (878/945)\n",
      "### Excluding FAEs:\n",
      "conditions: QueryImpl('and', frozenset({('>', ('attack_id',), 0), ('==', ('success',), 1)})) \n",
      "Overall TPR: 0.929101\tROC-AUC: 0.989578\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 0), ('>', ('attack_id',), 0)})) \n",
      "Overall detection rate on FAEs: 0.3161 \t  49/155\n",
      "Selected 0.000086 as the threshold value.\n",
      "\n",
      "\n",
      "Detector: FeatureSqueezing?squeezers=bit_depth_2&distance_measure=l1&fpr=0.05\n",
      "\n",
      "\n",
      "Accuracy: 0.861818\tTPR: 0.768382\tFPR: 0.046763\tROC-AUC: 0.891306\n",
      "Accuracy_all_test:0.960  FPR_all_test:0.040 For all TestSet\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 1), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.7391 \t  34/ 46 \t fgsm?eps=0.3\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 2)})) \n",
      "Detection rate on SAEs: 0.0870 \t   8/ 92 \t bim?eps=0.3&eps_iter=0.06\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 3), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t deepfool?overshoot=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 4)})) \n",
      "Detection rate on SAEs: 0.9400 \t  94/100 \t carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 5)})) \n",
      "Detection rate on SAEs: 0.9800 \t  98/100 \t carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 6), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.9596 \t  95/ 99 \t carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 7), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.9500 \t  95/100 \t carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 8), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.6300 \t  63/100 \t carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 9), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.7600 \t  76/100 \t carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 10)})) \n",
      "Detection rate on SAEs: 1.0000 \t  63/ 63 \t jsma?targeted=next\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 11), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t  45/ 45 \t jsma?targeted=ll\n",
      "945\n",
      "Overall detection rate on SAEs: 0.815873 (771/945)\n",
      "### Excluding FAEs:\n",
      "conditions: QueryImpl('and', frozenset({('>', ('attack_id',), 0), ('==', ('success',), 1)})) \n",
      "Overall TPR: 0.815873\tROC-AUC: 0.935661\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 0), ('>', ('attack_id',), 0)})) \n",
      "Overall detection rate on FAEs: 0.3742 \t  58/155\n",
      "Selected 0.002759 as the threshold value.\n",
      "\n",
      "\n",
      "Detector: FeatureSqueezing?squeezers=bit_depth_1,median_filter_2_2&distance_measure=l1&fpr=0.05\n",
      "\n",
      "\n",
      "Accuracy: 0.928182\tTPR: 0.900735\tFPR: 0.044964\tROC-AUC: 0.937164\n",
      "Accuracy_all_test:0.959  FPR_all_test:0.041 For all TestSet\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 1), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.9783 \t  45/ 46 \t fgsm?eps=0.3\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 2)})) \n",
      "Detection rate on SAEs: 0.9783 \t  90/ 92 \t bim?eps=0.3&eps_iter=0.06\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 3), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t deepfool?overshoot=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 4)})) \n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 5)})) \n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 6), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t  99/ 99 \t carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 7), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 8), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.9300 \t  93/100 \t carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 9), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 0.9500 \t  95/100 \t carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 1), ('==', ('attack_id',), 10)})) \n",
      "Detection rate on SAEs: 1.0000 \t  63/ 63 \t jsma?targeted=next\n",
      "\n",
      "\n",
      "conditions: QueryImpl('and', frozenset({('==', ('attack_id',), 11), ('==', ('success',), 1)})) \n",
      "Detection rate on SAEs: 1.0000 \t  45/ 45 \t jsma?targeted=ll\n",
      "945\n",
      "Overall detection rate on SAEs: 0.984127 (930/945)\n",
      "### Excluding FAEs:\n",
      "conditions: QueryImpl('and', frozenset({('>', ('attack_id',), 0), ('==', ('success',), 1)})) \n",
      "Overall TPR: 0.984127\tROC-AUC: 0.994110\n",
      "conditions: QueryImpl('and', frozenset({('==', ('success',), 0), ('>', ('attack_id',), 0)})) \n",
      "Overall detection rate on FAEs: 0.2903 \t  45/155\n"
     ]
    }
   ],
   "source": [
    "# if FLAGS.detection != '':\n",
    "#     # from detections.base import DetectionEvaluator\n",
    "\n",
    "#     result_folder_detection = os.path.join(FLAGS.result_folder, \"detection\")\n",
    "#     csv_fname = \"%s_attacks_%s_detection.csv\" % (task_id, attack_string_hash)\n",
    "#     de = DetectionEvaluator(model, result_folder_detection, csv_fname, FLAGS.dataset_name)\n",
    "#     Y_test_all_pred = model.predict(X_test_all)\n",
    "#     attack_string_list = filter(lambda x:len(x)>0, FLAGS.attacks.lower().split(';'))\n",
    "#     # de.build_detection_dataset(X_test_all, Y_test_all, Y_test_all_pred, selected_idx, X_test_adv_discretized_list, Y_test_adv_discretized_pred_list, attack_string_list, attack_string_hash, FLAGS.clip, Y_test_target_next, Y_test_target_ll)\n",
    "#     de.build_detection_dataset(X_test_all, Y_test_all, Y_test_all_pred, selected_idx, X_test_adv_discretized_list, Y_test_adv_discretized_pred_list,\n",
    "#                                 FLAGS.attacks, attack_string_hash, FLAGS.clip, Y_test_target_next, Y_test_target_ll)\n",
    "#     de.evaluate_detections(FLAGS.detection,X_test_all,Y_test_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCSVM-Siamese Detection method Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Defined TensorFlow model graph.\n",
      "---Loaded MNIST-carlini model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from numpy.linalg import norm\n",
    "model_1 = dataset.load_model_by_name(FLAGS.model_name, logits=False, input_range_type=1)\n",
    "model_1.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['acc'])\n",
    "\n",
    "carlini_model = keras.models.Model(inputs = model_1.input,\n",
    "                             outputs = model_1.layers[16].output)\n",
    "# carlini_model = keras.models.Model(inputs = model_1.input,\n",
    "#                              outputs = model_1.layers[16].output, name=\"Carlini_0\")\n",
    "# carlini_model.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/MNIST_Dataset/Sia_Triplets/MNIST_Carlini_Conv_model.h5\")\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "def triplet_loss(x, alpha = 1):\n",
    "# def triplet_loss(x, alpha = 10):\n",
    "# def triplet_loss(x, alpha = 30):\n",
    "    # Triplet Loss function.\n",
    "    anchor,positive,negative = x\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    "    return loss\n",
    "\n",
    "anchor_input   = layers.Input((28, 28, 1),name=\"anchor\"  )\n",
    "positive_input = layers.Input((28, 28, 1),name=\"positive\")    \n",
    "negative_input = layers.Input((28, 28, 1),name=\"negative\") \n",
    "\n",
    "normal_layer_1 = keras.layers.BatchNormalization(name=\"bat_0\")(carlini_model.output)\n",
    "normal_layer_1 = layers.Dense(256, activation=\"relu\")(normal_layer_1)\n",
    "normal_layer_1 = keras.layers.BatchNormalization(name=\"bat_1\")(normal_layer_1)\n",
    "normal_layer_1 = layers.Dense(256, activation=\"relu\")(normal_layer_1)\n",
    "normal_layer_1 = keras.layers.BatchNormalization(name=\"bat_2\")(normal_layer_1)\n",
    "\n",
    "embedding = Model(carlini_model.input, normal_layer_1, name=\"Embedding\")\n",
    "# embedding.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/MNIST_Dataset/Sia_Triplets_adv/Triplets_adv_10_embedding_weight.h5\")\n",
    "# embedding.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/MNIST_Dataset/Sia_Triplets_adv/Triplets_adv_1_embedding_weight.h5\")\n",
    "# embedding.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/MNIST_Dataset/Sia_Triplets_adv/Hard_Set/Triplets_adv_1_embedding_weight.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "anchor (InputLayer)              (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "positive (InputLayer)            (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "negative (InputLayer)            (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "Embedding (Model)                (None, 256)           430288      anchor[0][0]                     \n",
      "                                                                   positive[0][0]                   \n",
      "                                                                   negative[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (None,)               0           Embedding[1][0]                  \n",
      "                                                                   Embedding[2][0]                  \n",
      "                                                                   Embedding[3][0]                  \n",
      "====================================================================================================\n",
      "Total params: 430,288\n",
      "Trainable params: 428,864\n",
      "Non-trainable params: 1,424\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "A = embedding(anchor_input  )\n",
    "P = embedding(positive_input)\n",
    "N = embedding(negative_input)\n",
    "\n",
    "loss = layers.Lambda(triplet_loss)([A, P, N])\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=loss )\n",
    "siamese_network.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/MNIST_Dataset/Sia_Triplets_adv/Hard_Set/ep_alpha1_03_los0.033_MNIST_Triplets_adv.h5\")#Best_Result\n",
    "# siamese_network.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/MNIST_Dataset/Sia_Triplets_64/MNIST_Triplets_MedClahe_64.h5\")\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\spd\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator OneClassSVM from version 1.3.0 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "dim          = 64\n",
    "IMG_WIDTH    = dim\n",
    "IMG_HEIGHT   = dim\n",
    "IMG_CHANNELS = 1\n",
    "inputs = keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "# s = keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "#Contraction path\n",
    "c1 = keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "b1 = BatchNormalization()(c1)\n",
    "c1 = keras.layers.Dropout(0.1)(b1)\n",
    "c1 = keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "b1 = BatchNormalization()(c1)\n",
    "p1 = keras.layers.MaxPooling2D((2, 2))(b1)\n",
    "\n",
    "c2 = keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "b2 = BatchNormalization()(c2)\n",
    "c2 = keras.layers.Dropout(0.1)(b2)\n",
    "c2 = keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "b2 = BatchNormalization()(c2)\n",
    "p2 = keras.layers.MaxPooling2D((2, 2))(b2)\n",
    "\n",
    "c3 = keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "b3 = BatchNormalization()(c3)\n",
    "c3 = keras.layers.Dropout(0.2)(b3)\n",
    "c3 = keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "b3 = BatchNormalization()(c3)\n",
    "p3 = keras.layers.MaxPooling2D((2, 2))(b3)\n",
    "\n",
    "c4 = keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "b4 = BatchNormalization()(c4)\n",
    "c4 = keras.layers.Dropout(0.2)(b4)\n",
    "c4 = keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "b4 = BatchNormalization()(c4)\n",
    "p4 = keras.layers.MaxPooling2D(pool_size=(2, 2))(b4)\n",
    "\n",
    "c5 = keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "b5 = BatchNormalization()(c5)\n",
    "c5 = keras.layers.Dropout(0.3)(b5)\n",
    "c5 = keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "b5 = BatchNormalization()(c5)\n",
    "\n",
    "#Expansive path \n",
    "u6 = keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(b5)\n",
    "u6 = keras.layers.concatenate([u6, c4])\n",
    "c6 = keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "b6 = BatchNormalization()(c6)\n",
    "c6 = keras.layers.Dropout(0.2)(b6)\n",
    "c6 = keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "b6 = BatchNormalization()(c6)\n",
    "\n",
    "u7 = keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(b6)\n",
    "u7 = keras.layers.concatenate([u7, c3])\n",
    "c7 = keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "b7 = BatchNormalization()(c7)\n",
    "c7 = keras.layers.Dropout(0.2)(b7)\n",
    "c7 = keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "b7 = BatchNormalization()(c7)\n",
    "\n",
    "u8 = keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(b7)\n",
    "u8 = keras.layers.concatenate([u8, c2])\n",
    "c8 = keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "b8 = BatchNormalization()(c8)\n",
    "c8 = keras.layers.Dropout(0.1)(b8)\n",
    "c8 = keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "b8 = BatchNormalization()(c8)\n",
    "\n",
    "u9 = keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(b8)\n",
    "u9 = keras.layers.concatenate([u9, c1], axis=3)\n",
    "c9 = keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "b9 = BatchNormalization()(c9)\n",
    "c9 = keras.layers.Dropout(0.1)(b9)\n",
    "c9 = keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "b9 = BatchNormalization()(c9)\n",
    "\n",
    "outputs = keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(b9)\n",
    "\n",
    "U_Net_Model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
    "U_Net_Model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# self.U_Net_Model.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/MNIST_Dataset/checkpointsUnet/MNIST_Unet_first_10_ep_plus_10_all.h5\")\n",
    "U_Net_Model.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/MNIST_Dataset/checkpointsUnet/allTset_ep_30_acc0.97981_MNIST_UNet.h5\")\n",
    "# self.U_Net_Model.load_weights(\"MNIST_U_Net_with_DC.h5\")\n",
    "#Load One Class SVM\n",
    "filename = 'C:/Users/ahmad/Desktop/JupyterFilesAUB/MNIST_Dataset/checkpointsSVM/one_svm_model_MNIST_.sav'\n",
    "# filename   = 'One_SVM_MNIST_model_DC.sav'\n",
    "OC_SVM_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering...\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "train_images = medfilter(train_images)\n",
    "# test_images  = medfilter(test_images )\n",
    "# train_images = train_images.astype(np.float32)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_VFlip = []\n",
    "# for i in range(len(train_images)):\n",
    "#     train_images_VFlip.append(cv2.flip(train_images[i], 1))\n",
    "# train_images_VFlip = np.array(train_images_VFlip)\n",
    "# train_images_VFlip = train_images_VFlip.astype(np.float32)/255\n",
    "train_images       = train_images.astype      (np.float32)/255\n",
    "# train_images_VFlip = np.expand_dims(train_images_VFlip, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_test,Y_test,threshold, flag):\n",
    "        adv_cls = 10\n",
    "        x_adv_SVM = np.zeros(len(Y_test))\n",
    "        X_test_ = []\n",
    "        for img in X_test:\n",
    "            image = cv2.resize(img, (dim,dim)) #64*64*1 Images\n",
    "            img_uint8 = np.clip(np.rint(image * 255), 0, 255).astype(np.uint8)\n",
    "            image = np.expand_dims(img_uint8, axis=2)\n",
    "            X_test_.append(image)\n",
    "        X_test_ = np.array(X_test_)\n",
    "        X_test_U = X_test_/255.\n",
    "        X_test_mask           = U_Net_Model.predict(X_test_U,verbose=0)\n",
    "        X_test_preds_mask_t   = (X_test_mask > 0.5).astype(np.uint8)\n",
    "        X_test_masks          = np.array(X_test_preds_mask_t)\n",
    "        adv_labels_SVM = x_adv_SVM\n",
    "        X_adv_OCsvm = X_test_masks.reshape((len(X_test_masks), -1))\n",
    "        result = OC_SVM_model.predict(X_adv_OCsvm )\n",
    "        oc_indices = np.where(result==-1)[0]\n",
    "        adv_labels_SVM[oc_indices] = adv_cls\n",
    "#############################################################\n",
    "        Y_test  = np.array(Y_test )\n",
    "        X_test_orig       = []\n",
    "        X_test_orig_flip  = []\n",
    "        for img in X_test:\n",
    "            img_uint8 = np.clip(np.rint(img * 255), 0, 255).astype(np.uint8)\n",
    "            X_test_orig.append(img_uint8)\n",
    "        X_test_orig = medfilter(X_test_orig )\n",
    "        for i in range(len(X_test_orig)):\n",
    "            X_test_orig_flip.append(cv2.flip(X_test_orig[i], 1))\n",
    "        X_test_orig      = np.array(X_test_orig)\n",
    "        # X_test_orig_flip = np.array(X_test_orig_flip)\n",
    "        # X_test_orig_flip = np.expand_dims(X_test_orig_flip, axis=3)\n",
    "        # X_test_orig = X_test_orig.reshape(X_test_orig.shape[0],X_test_orig.shape[1],X_test_orig.shape[2],1)\n",
    "        # X_test_orig  = X_test_orig.astype(np.float32)/255\n",
    "        # print(X_test_orig.shape)\n",
    "        num_classes_clean   = max(train_labels) + 1\n",
    "        digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n",
    "        siamese_labels_1    = []\n",
    "        for idx1 in range(len(X_test_orig)):\n",
    "            # add a matching example\n",
    "            x1     = X_test_orig [idx1]\n",
    "            label1 = Y_test      [idx1]\n",
    "            x1     = x1/255.\n",
    "            idx2   = random.choice(digit_indices_clean[label1])\n",
    "            x2     = train_images[idx2]\n",
    "            idx3   = random.choice(digit_indices_clean[label1])\n",
    "            x3     = train_images[idx3]\n",
    "            idx4   = random.choice(digit_indices_clean[label1])\n",
    "            x4     = train_images[idx4]#Three_Sample\n",
    "\n",
    "            anchor   = embedding.predict(np.expand_dims(x1, axis=0),verbose = 0)\n",
    "            positive = embedding.predict(np.expand_dims(x2, axis=0),verbose = 0)\n",
    "            positive3 = embedding.predict(np.expand_dims(x3, axis=0),verbose = 0)\n",
    "            # positive4 = embedding.predict(np.expand_dims(x4, axis=0),verbose = 0)#Three_Sample\n",
    "\n",
    "            anchor   = np.reshape(anchor,256)\n",
    "            positive = np.reshape(positive,256)\n",
    "            positive3 = np.reshape(positive3,256)\n",
    "            positive4 = np.reshape(positive4,256)\n",
    "            pos_dist = np.dot(anchor,positive)/(norm(anchor)*norm(positive))\n",
    "            pos_dist3 = np.dot(anchor,positive3)/(norm(anchor)*norm(positive3))\n",
    "            # pos_dist4 = np.dot(anchor,positive4)/(norm(anchor)*norm(positive4))#Three_Sample\n",
    "            # print(type(float(pos_dist)),pos_dist)\n",
    "            pos_dist_Final = min(pos_dist,pos_dist3)#Two_Sample\n",
    "            # pos_dist_Final = min(pos_dist,pos_dist3,pos_dist4)#Three_Sample\n",
    "            siamese_labels_1.append(float(pos_dist_Final))\n",
    "\n",
    "\n",
    "        print(len(siamese_labels_1))\n",
    "        print(siamese_labels_1[0])\n",
    "        if len(siamese_labels_1) !=0:\n",
    "            x_ = np.zeros(len(siamese_labels_1))\n",
    "            # y_= np.zeros(len(siamese_labels_1))\n",
    "            x_[np.where(np.array(siamese_labels_1)>threshold)[0]] = 1\n",
    "            # print(x_)\n",
    "            # y_[np.where(np.array(siamese_labels_2)>threshold)[0]] = 1\n",
    "            # print(y_)\n",
    "            # x_.astype(bool)\n",
    "            # x_ = x_.astype(bool) & y_.astype(bool)\n",
    "            # x_ = x_*1\n",
    "            res = len(np.where(x_==0)[0])/len(siamese_labels_1)\n",
    "            print(\"Siamese_Detection\",res,\"%\")\n",
    "            x_[oc_indices] = 0     #SVM Detection\n",
    "            return x_== 0 # inverse logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Segmentation\n",
      "['detector', 'threshold', 'fpr', 'fpr_all_test_clean', 'Accuracy_all_test_clean', 'fgsm?eps=0.3', 'bim?eps=0.3&eps_iter=0.06', 'deepfool?overshoot=10', 'carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10', 'carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10', 'carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10', 'carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10', 'carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10', 'carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10', 'jsma?targeted=next', 'jsma?targeted=ll', 'overall']\n",
      "Filtering...\n",
      "100\n",
      "0.9365708231925964\n",
      "Siamese_Detection 0.03 %\n",
      "Detector: MNIST_Sia_Triplets_Three_sample_HardSet_SVM_Alpha1_Med_Clahe\n",
      "Accuracy: 0.970000\tTPR: nan\tFPR: 0.030000\t\n",
      "Filtering...\n",
      "10000\n",
      "0.956290602684021\n",
      "Siamese_Detection 0.066 %\n",
      "Accuracy_all_test:0.926  FPR_all_test:0.074 For all TestSet\n",
      "Filtering...\n",
      "46\n",
      "0.46892330050468445\n",
      "Siamese_Detection 0.43478260869565216 %\n",
      "Detection rate on SAEs: 1.0000 \t  46/ 46 \t fgsm?eps=0.3\n",
      "overall_detection_rate_saes/nb_saes: 46/46\n",
      "Filtering...\n",
      "92\n",
      "0.3892748951911926\n",
      "Siamese_Detection 0.32608695652173914 %\n",
      "Detection rate on SAEs: 1.0000 \t  92/ 92 \t bim?eps=0.3&eps_iter=0.06\n",
      "overall_detection_rate_saes/nb_saes: 138/138\n",
      "Filtering...\n",
      "100\n",
      "-0.2727469205856323\n",
      "Siamese_Detection 0.8 %\n",
      "Detection rate on SAEs: 0.9000 \t  90/100 \t deepfool?overshoot=10\n",
      "overall_detection_rate_saes/nb_saes: 228/238\n",
      "Yes Next\n",
      "Filtering...\n",
      "100\n",
      "-0.08033188432455063\n",
      "Siamese_Detection 0.84 %\n",
      "Detection rate on SAEs: 0.9600 \t  96/100 \t carlinili?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "overall_detection_rate_saes/nb_saes: 324/338\n",
      "Yes LL\n",
      "Filtering...\n",
      "100\n",
      "-0.514341413974762\n",
      "Siamese_Detection 0.91 %\n",
      "Detection rate on SAEs: 0.9700 \t  97/100 \t carlinili?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "overall_detection_rate_saes/nb_saes: 421/438\n",
      "Yes Next\n",
      "Filtering...\n",
      "99\n",
      "-0.2237367331981659\n",
      "Siamese_Detection 0.9292929292929293 %\n",
      "Detection rate on SAEs: 0.9495 \t  94/ 99 \t carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=10\n",
      "overall_detection_rate_saes/nb_saes: 515/537\n",
      "Yes LL\n",
      "Filtering...\n",
      "100\n",
      "-0.4812491834163666\n",
      "Siamese_Detection 0.96 %\n",
      "Detection rate on SAEs: 0.9600 \t  96/100 \t carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=10\n",
      "overall_detection_rate_saes/nb_saes: 611/637\n",
      "Yes Next\n",
      "Filtering...\n",
      "100\n",
      "-0.11923881620168686\n",
      "Siamese_Detection 0.93 %\n",
      "Detection rate on SAEs: 0.9400 \t  94/100 \t carlinil0?targeted=next&batch_size=1&max_iterations=1000&confidence=10\n",
      "overall_detection_rate_saes/nb_saes: 705/737\n",
      "Yes LL\n",
      "Filtering...\n",
      "100\n",
      "-0.5168285369873047\n",
      "Siamese_Detection 0.99 %\n",
      "Detection rate on SAEs: 0.9900 \t  99/100 \t carlinil0?targeted=ll&batch_size=1&max_iterations=1000&confidence=10\n",
      "overall_detection_rate_saes/nb_saes: 804/837\n",
      "Yes Next\n",
      "Filtering...\n",
      "63\n",
      "0.546454131603241\n",
      "Siamese_Detection 0.9523809523809523 %\n",
      "Detection rate on SAEs: 0.9524 \t  60/ 63 \t jsma?targeted=next\n",
      "overall_detection_rate_saes/nb_saes: 864/900\n",
      "Yes LL\n",
      "Filtering...\n",
      "45\n",
      "-0.5908560156822205\n",
      "Siamese_Detection 0.9777777777777777 %\n",
      "Detection rate on SAEs: 0.9778 \t  44/ 45 \t jsma?targeted=ll\n",
      "overall_detection_rate_saes/nb_saes: 908/945\n",
      "945\n",
      "Overall detection rate on SAEs: 0.960847 (908/945)\n",
      "Filtering...\n",
      "155\n",
      "0.9259593486785889\n",
      "Siamese_Detection 0.16774193548387098 %\n",
      "Overall detection rate on FAEs: 0.4903 \t  76/155\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello Segmentation\")\n",
    "# detector_names = [ele.strip() for ele in params_str.split(';') if ele.strip()!= '']\n",
    "attack_names = FLAGS.attacks.lower().split(';')\n",
    "# dataset_name = \"MNIST\"\n",
    "# detector_name = \"MNIST_Sia_Triplets_Alpha10_filt_SVM\"\n",
    "# csv_fpath = \"./MNIST_Sia_Triplets_Med_Clahe_Alpha10_filt_SVM_detection_%s_saes_point5_thresh.csv\" % dataset_name \n",
    "# fieldnames = ['detector', 'threshold', 'fpr','fpr_all_test_clean','Accuracy_all_test_clean'] + list(attack_names) + ['overall']\n",
    "dataset_name = \"MNIST\"\n",
    "# detector_name = \"MNIST_Sia_Triplets_Flip_only_SVM_Alpha10_Med_Clahe\"\n",
    "# csv_fpath = \"./MNIST_Sia_Triplets_Flip_only_SVM_Med_Clahe_Alpha10_detection_%s_saes_point5_thresh.csv\" % dataset_name \n",
    "# detector_name = \"MNIST_Sia_Triplets_HardSet_SVM_Alpha1_Med_Clahe\"\n",
    "# csv_fpath = \"./MNIST_Sia_Triplets_HardSet_SVM_Med_Clahe_Alpha1_detection_%s_saes_point5_thresh.csv\" % dataset_name \n",
    "# detector_name = \"MNIST_Sia_Triplets_Two_sample_HardSet_SVM_Alpha1_Med_Clahe\"\n",
    "# csv_fpath = \"./MNIST_Sia_Triplets_Two_sample_HardSet_SVM_Med_Clahe_Alpha1_detection_%s_saes_point5_thresh.csv\" % dataset_name \n",
    "detector_name = \"MNIST_Sia_Triplets_Three_sample_HardSet_SVM_Alpha1_Med_Clahe\"\n",
    "csv_fpath = \"./MNIST_Sia_Triplets_Three_sample_HardSet_SVM_Med_Clahe_Alpha1_detection_%s_saes_point5_thresh.csv\" % dataset_name \n",
    "fieldnames = ['detector', 'threshold', 'fpr','fpr_all_test_clean','Accuracy_all_test_clean'] + list(attack_names) + ['overall']\n",
    "print(fieldnames)\n",
    "to_csv = []\n",
    "threshold = 0.5\n",
    "flag = 0\n",
    "\n",
    "Y_test_pred  = test(X_test_all[:len(selected_idx)],np.argmax(Y_test_all[:len(selected_idx)],axis=1),threshold, flag)\n",
    "X_test_zeros = np.zeros(np.array(Y_test_pred).shape) \n",
    "accuracy, tpr, fpr, tp, ap = evalulate_detection_test(X_test_zeros, Y_test_pred)\n",
    "# fprs, tprs, thresholds = roc_curve(Y_test, Y_test_pred_score)\n",
    "# roc_auc = auc(fprs, tprs)\n",
    "\n",
    "print (\"Detector: %s\" % detector_name)\n",
    "print (\"Accuracy: %f\\tTPR: %f\\tFPR: %f\\t\" % (accuracy, tpr, fpr))\n",
    "\n",
    "Y_test_pred_t = test(X_test_all,np.argmax(Y_test_all,axis=1),threshold, flag)\n",
    "# print(Y_test_pred_t[0], \"Hello Boy\" )\n",
    "Y_test_thresh = np.zeros(np.array(np.argmax(Y_test_all,axis=1)).shape) \n",
    "accuracy_all, tpr_all, fpr_all, tp_all, ap_all = evalulate_detection_test(Y_test_thresh, Y_test_pred_t)\n",
    "print(\"Accuracy_all_test:%.3f\" % accuracy_all,\" FPR_all_test:%.3f\" % fpr_all,\"For all TestSet\")\n",
    "flag = 1\n",
    "rec  = {}\n",
    "rec['detector'] = detector_name\n",
    "rec['threshold'] = threshold\n",
    "rec['fpr'] = fpr\n",
    "rec['Accuracy_all_test_clean'] = accuracy_all\n",
    "rec['fpr_all_test_clean'] = fpr_all\n",
    "overall_detection_rate_saes = 0\n",
    "nb_saes = 0\n",
    "\n",
    "count = 0\n",
    "y_check_non = []\n",
    "x_check_non = []\n",
    "for attack_name in attack_names:\n",
    "    if 'targeted=ll' in attack_name:\n",
    "        print(\"Yes LL\")\n",
    "        Y_adv_orig  = np.argmax(Y_test_adv_discretized_pred_list[count],axis=1)\n",
    "        adv_indeces = np.where (Y_adv_orig==np.argmax(Y_test_target_ll, axis=1))[0]\n",
    "        y_check     = Y_adv_orig[adv_indeces]\n",
    "        Y_test_pred       = test(np.array(X_test_adv_discretized_list[count])[adv_indeces],\n",
    "                                     y_check,threshold, flag)\n",
    "        non_adv_indeces = np.where (Y_adv_orig!=np.argmax(Y_test_target_ll, axis=1))[0]\n",
    "        if len(non_adv_indeces)>0:\n",
    "            y_check_non.append(Y_adv_orig[non_adv_indeces])\n",
    "            x_check_non.append(np.array(X_test_adv_discretized_list[count])[non_adv_indeces])\n",
    "        count+=1\n",
    "    elif 'targeted=next' in attack_name:\n",
    "        print(\"Yes Next\")\n",
    "        Y_adv_orig  = np.argmax(Y_test_adv_discretized_pred_list[count],axis=1)\n",
    "        adv_indeces = np.where (Y_adv_orig==np.argmax(Y_test_target_next, axis=1))[0]\n",
    "        y_check     = Y_adv_orig[adv_indeces]\n",
    "        Y_test_pred       = test(np.array(X_test_adv_discretized_list[count])[adv_indeces],\n",
    "                                   y_check ,threshold, flag)\n",
    "        non_adv_indeces = np.where (Y_adv_orig!=np.argmax(Y_test_target_next, axis=1))[0]\n",
    "        if len(non_adv_indeces)>0:\n",
    "            y_check_non.append(Y_adv_orig[non_adv_indeces])\n",
    "            x_check_non.append(np.array(X_test_adv_discretized_list[count])[non_adv_indeces])\n",
    "        count+=1\n",
    "    else:\n",
    "        Y_adv_orig  = np.argmax(Y_test_adv_discretized_pred_list[count],axis=1)\n",
    "        adv_indeces = np.where (Y_adv_orig!=np.argmax(Y_test_all[selected_idx], axis=1))[0]\n",
    "        y_check     = Y_adv_orig[adv_indeces]\n",
    "        Y_test_pred = test(np.array(X_test_adv_discretized_list[count])[adv_indeces],\n",
    "                                    y_check,threshold, flag)\n",
    "        non_adv_indeces = np.where (Y_adv_orig==np.argmax(Y_test_all[selected_idx], axis=1))[0]\n",
    "        if len(non_adv_indeces)>0:\n",
    "            y_check_non.append(Y_adv_orig[non_adv_indeces])\n",
    "            x_check_non.append(np.array(X_test_adv_discretized_list[count])[non_adv_indeces])\n",
    "        count+=1\n",
    "    Y_sae             = np.ones(np.array(Y_test_pred).shape) \n",
    "    _, tpr, _, tp, ap = evalulate_detection_test(Y_sae, Y_test_pred)\n",
    "    print (\"Detection rate on SAEs: %.4f \\t %3d/%3d \\t %s\" % (tpr, tp, ap, attack_name))\n",
    "    overall_detection_rate_saes += tpr * len(Y_sae)\n",
    "    nb_saes += len(Y_sae)\n",
    "    rec[attack_name] = tpr\n",
    "    print (\"overall_detection_rate_saes/nb_saes: %d/%d\" % (overall_detection_rate_saes, nb_saes))\n",
    "    # break\n",
    "print(nb_saes)\n",
    "print (\"Overall detection rate on SAEs: %f (%d/%d)\" % (overall_detection_rate_saes/nb_saes, overall_detection_rate_saes, nb_saes))\n",
    "rec['overall'] = float(overall_detection_rate_saes/nb_saes)\n",
    "to_csv.append(rec)\n",
    "y_check_non = np.array(list(itertools.chain(*y_check_non)))\n",
    "x_check_non = np.array(list(itertools.chain(*x_check_non)))\n",
    "#FAEs\n",
    "Y_test_pred = test(x_check_non,\n",
    "                                   y_check_non ,threshold, flag)\n",
    "Y_fae       = np.ones(np.array(y_check_non).shape) \n",
    "_, tpr, _, tp, ap = evalulate_detection_test(Y_fae, Y_test_pred)\n",
    "print (\"Overall detection rate on FAEs: %.4f \\t %3d/%3d\" % (tpr, tp, ap))\n",
    "write_to_csv(to_csv, csv_fpath, fieldnames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
