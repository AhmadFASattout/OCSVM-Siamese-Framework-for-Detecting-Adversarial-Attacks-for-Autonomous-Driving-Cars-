{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Located Cleverhans\n",
      "Located Carlini_nn_robust_attacks\n",
      "Located Keras-deep-learning-models\n",
      "Located MobileNets\n",
      "Located Deepfool/Universal\n",
      "Located DenseNet\n",
      "Located MagNet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x18a2f09c790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "# from  svhn import SVHNDataset\n",
    "from datasets_utils import *\n",
    "from cifar10_tensorenv import CIFAR10Dataset\n",
    "from mnist import MNISTDataset\n",
    "# from imagenet import ImageNetDataset\n",
    "\n",
    "from datasets_utils import get_correct_prediction_idx, evaluate_adversarial_examples, calculate_mean_confidence, calculate_accuracy\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "from skimage import img_as_ubyte\n",
    "import keras\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.python.platform import app\n",
    "app.flags.DEFINE_string('f', '', 'kernel')\n",
    "from tensorflow.python.platform import flags\n",
    "import itertools\n",
    "FLAGS = flags.FLAGS\n",
    "from CIFAR_10_Util_split_MedFilter_Clahe import medfilter\n",
    "flags.DEFINE_string('dataset_name', 'MNIST', 'Supported: MNIST, CIFAR-10, ImageNet, SVHN.')\n",
    "flags.DEFINE_string('model_name', 'cleverhans', 'Supported: cleverhans, cleverhans_adv_trained and carlini for MNIST; carlini and DenseNet for CIFAR-10;  ResNet50, VGG19, Inceptionv3 and MobileNet for ImageNet; tohinz for SVHN.')\n",
    "flags.DEFINE_boolean('select', True, 'Select correctly classified examples for the experiement.')\n",
    "flags.DEFINE_boolean('balance_sampling', False, 'Select the same number of examples for each class.')\n",
    "flags.DEFINE_boolean('test_mode', False, 'Only select one sample for each class.')\n",
    "flags.DEFINE_integer('nb_examples', 100, 'The number of examples selected for attacks.')\n",
    "flags.DEFINE_string('result_folder', \"results\", 'The output folder for results.')\n",
    "flags.DEFINE_string('attacks',\"FGSM?eps=0.1;BIM?eps=0.1&eps_iter=0.02;JSMA?targeted=next;CarliniL2?targeted=next&batch_size=100&max_iterations=1000;CarliniL2?targeted=next&batch_size=100&max_iterations=1000&confidence=2\", 'Attack name and parameters in URL style, separated by semicolon.')\n",
    "flags.DEFINE_float('clip', -1, 'L-infinity clip on the adversarial perturbations.')\n",
    "flags.DEFINE_boolean('visualize', True, 'Output the image examples for each attack, enabled by default.')\n",
    "flags.DEFINE_boolean('verbose', False, 'Stdout level. The hidden content will be saved to log files anyway.')\n",
    "# flags.DEFINE_string('detection', '', 'Supported: feature_squeezing.')\n",
    "flags.DEFINE_string('detection', \"FeatureSqueezing?squeezers=bit_depth_1&distance_measure=l1&fpr=0.05;FeatureSqueezing?squeezers=bit_depth_2&distance_measure=l1&fpr=0.05;FeatureSqueezing?squeezers=bit_depth_1,median_filter_2_2&distance_measure=l1&fpr=0.05;\", 'Supported: feature_squeezing.')\n",
    "flags.DEFINE_boolean('detection_train_test_mode', False, 'Split into train/test datasets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAGS.model_name     = 'cleverhans'\n",
    "FLAGS.dataset_name     = 'CIFAR-10'\n",
    "# FLAGS.model_name       = 'carlini'\n",
    "FLAGS.model_name       = 'densenet'\n",
    "FLAGS.select           = True\n",
    "FLAGS.balance_sampling = True\n",
    "FLAGS.test_mode        = False\n",
    "FLAGS.nb_examples      = 100\n",
    "FLAGS.result_folder    = \"results\"\n",
    "# FLAGS.attacks          =\"carlinili?targeted=next&batch_size=100&max_iterations=1000&confidence=10;\"\n",
    "# FLAGS.detection        =\"FeatureSqueezing?squeezers=bit_depth_1&distance_measure=l1&fpr=0.05;\\\n",
    "# FeatureSqueezing?squeezers=bit_depth_2&distance_measure=l1&fpr=0.05;\\\n",
    "# FeatureSqueezing?squeezers=bit_depth_1,median_filter_2_2&distance_measure=l1&fpr=0.05;\"\n",
    "FLAGS.attacks         = \\\n",
    "\"fgsm?eps=0.0156;bim?eps=0.008&eps_iter=0.0012;\\\n",
    "carlinili?targeted=next&confidence=5;\\\n",
    "carlinili?targeted=ll&confidence=5;\\\n",
    "deepfool?overshoot=10;\\\n",
    "carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=5;\\\n",
    "carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=5;\\\n",
    "carlinil0?targeted=next&confidence=5;carlinil0?targeted=ll&confidence=5;\\\n",
    "jsma?targeted=next;\\\n",
    "jsma?targeted=ll\" \n",
    "# FLAGS.attacks         = \"FGSM?eps=0.1;BIM?eps=0.1&eps_iter=0.02;JSMA?targeted=next;CarliniL2?targeted=next&batch_size=100&max_iterations=1000;\"\n",
    "# FLAGS.attacks         = \"fgsm?eps=0.0156;bim?eps=0.008&eps_iter=0.0012;carlinili?targeted=next&confidence=5;carlinili?targeted=ll&confidence=5;deepfool?overshoot=10;carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=5;carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=5;carlinil0?targeted=next&confidence=5;carlinil0?targeted=ll&confidence=5;jsma?targeted=next;jsma?targeted=ll;\"\n",
    "FLAGS.detection = \\\n",
    "\"FeatureSqueezing?squeezers=bit_depth_1&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=bit_depth_2&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=bit_depth_3&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=bit_depth_4&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=bit_depth_5&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=median_filter_2_2&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=median_filter_3_3&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=non_local_means_color_11_3_2&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=non_local_means_color_11_3_4&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=non_local_means_color_13_3_2&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=non_local_means_color_13_3_4&distance_measure=l1&fpr=0.05;\\\n",
    "FeatureSqueezing?squeezers=bit_depth_5,median_filter_2_2,non_local_means_color_13_3_2&distance_measure=l1&fpr=0.05;\"\n",
    "# FLAGS.clip             = -1\n",
    "FLAGS.visualize        = True\n",
    "FLAGS.verbose          = False\n",
    "\n",
    "def load_tf_session():\n",
    "    # Set TF random seed to improve reproducibility\n",
    "    tf.random.set_seed(1234)\n",
    "    # tf.set_random_seed(1234)\n",
    "\n",
    "    # Create TF session and set as Keras backend session\n",
    "    sess = tf.compat.v1.Session()\n",
    "    # sess = tf.Session()\n",
    "    keras.backend.set_session(sess)\n",
    "    print(\"Created TensorFlow session and set Keras backend.\")\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Loading CIFAR-10 data...\n",
      "Created TensorFlow session and set Keras backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\keras\\backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Defined TensorFlow model graph.\n",
      "---Loaded CIFAR-10-densenet model.\n",
      "\n",
      "Model: \"densenet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " initial_conv2D (Conv2D)        (None, 32, 32, 16)   432         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['initial_conv2D[0][0]']         \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 12)   1728        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32, 32, 28)   0           ['initial_conv2D[0][0]',         \n",
      "                                                                  'conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 28)  112         ['concatenate[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 28)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 12)   3024        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 40)   0           ['concatenate[0][0]',            \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 40)  160         ['concatenate_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 40)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 12)   4320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 52)   0           ['concatenate_1[0][0]',          \n",
      "                                                                  'conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 52)  208         ['concatenate_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 52)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 12)   5616        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 64)   0           ['concatenate_2[0][0]',          \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 64)  256         ['concatenate_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 12)   6912        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 76)   0           ['concatenate_3[0][0]',          \n",
      "                                                                  'conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 76)  304         ['concatenate_4[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 76)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 12)   8208        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 32, 88)   0           ['concatenate_4[0][0]',          \n",
      "                                                                  'conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 88)  352         ['concatenate_5[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 88)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 12)   9504        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 100)  0           ['concatenate_5[0][0]',          \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 100)  400        ['concatenate_6[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 100)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 12)   10800       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 32, 32, 112)  0           ['concatenate_6[0][0]',          \n",
      "                                                                  'conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 32, 32, 112)  448        ['concatenate_7[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 32, 32, 112)  0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 12)   12096       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 32, 32, 124)  0           ['concatenate_7[0][0]',          \n",
      "                                                                  'conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 32, 32, 124)  496        ['concatenate_8[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 32, 32, 124)  0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 12)   13392       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 32, 32, 136)  0           ['concatenate_8[0][0]',          \n",
      "                                                                  'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 32, 32, 136)  544        ['concatenate_9[0][0]']          \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 32, 32, 136)  0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 12)   14688       ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 32, 32, 148)  0           ['concatenate_9[0][0]',          \n",
      "                                                                  'conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 32, 32, 148)  592        ['concatenate_10[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 32, 32, 148)  0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 12)   15984       ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 32, 32, 160)  0           ['concatenate_10[0][0]',         \n",
      "                                                                  'conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 32, 32, 160)  640        ['concatenate_11[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 32, 32, 160)  0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 160)  25600       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 160)  0          ['conv2d_12[0][0]']              \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 160)  640        ['average_pooling2d[0][0]']      \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 160)  0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 12)   17280       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 16, 16, 172)  0           ['average_pooling2d[0][0]',      \n",
      "                                                                  'conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 172)  688        ['concatenate_12[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 172)  0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 16, 16, 12)   18576       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 16, 16, 184)  0           ['concatenate_12[0][0]',         \n",
      "                                                                  'conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 184)  736        ['concatenate_13[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 16, 16, 184)  0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 12)   19872       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 16, 16, 196)  0           ['concatenate_13[0][0]',         \n",
      "                                                                  'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 196)  784        ['concatenate_14[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 16, 16, 196)  0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 12)   21168       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_15 (Concatenate)   (None, 16, 16, 208)  0           ['concatenate_14[0][0]',         \n",
      "                                                                  'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 208)  832        ['concatenate_15[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 16, 16, 208)  0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 12)   22464       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 16, 16, 220)  0           ['concatenate_15[0][0]',         \n",
      "                                                                  'conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 16, 16, 220)  880        ['concatenate_16[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 16, 16, 220)  0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 12)   23760       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 16, 16, 232)  0           ['concatenate_16[0][0]',         \n",
      "                                                                  'conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 16, 16, 232)  928        ['concatenate_17[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 16, 16, 232)  0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 12)   25056       ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 16, 16, 244)  0           ['concatenate_17[0][0]',         \n",
      "                                                                  'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 16, 16, 244)  976        ['concatenate_18[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 16, 16, 244)  0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 12)   26352       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenate)   (None, 16, 16, 256)  0           ['concatenate_18[0][0]',         \n",
      "                                                                  'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 16, 16, 256)  1024       ['concatenate_19[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 16, 16, 256)  0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 12)   27648       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenate)   (None, 16, 16, 268)  0           ['concatenate_19[0][0]',         \n",
      "                                                                  'conv2d_21[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 16, 16, 268)  1072       ['concatenate_20[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 16, 16, 268)  0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 12)   28944       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 16, 16, 280)  0           ['concatenate_20[0][0]',         \n",
      "                                                                  'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 16, 16, 280)  1120       ['concatenate_21[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 16, 16, 280)  0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 12)   30240       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 16, 16, 292)  0           ['concatenate_21[0][0]',         \n",
      "                                                                  'conv2d_23[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 16, 16, 292)  1168       ['concatenate_22[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 16, 16, 292)  0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 12)   31536       ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_23 (Concatenate)   (None, 16, 16, 304)  0           ['concatenate_22[0][0]',         \n",
      "                                                                  'conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 16, 16, 304)  1216       ['concatenate_23[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 16, 16, 304)  0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 304)  92416       ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 8, 8, 304)   0           ['conv2d_25[0][0]']              \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 8, 8, 304)   1216        ['average_pooling2d_1[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 8, 8, 304)    0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 8, 8, 12)     32832       ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_24 (Concatenate)   (None, 8, 8, 316)    0           ['average_pooling2d_1[0][0]',    \n",
      "                                                                  'conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 8, 8, 316)   1264        ['concatenate_24[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 8, 8, 316)    0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 8, 8, 12)     34128       ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_25 (Concatenate)   (None, 8, 8, 328)    0           ['concatenate_24[0][0]',         \n",
      "                                                                  'conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 8, 8, 328)   1312        ['concatenate_25[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 8, 8, 328)    0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 8, 8, 12)     35424       ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_26 (Concatenate)   (None, 8, 8, 340)    0           ['concatenate_25[0][0]',         \n",
      "                                                                  'conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 8, 8, 340)   1360        ['concatenate_26[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 8, 8, 340)    0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 8, 8, 12)     36720       ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_27 (Concatenate)   (None, 8, 8, 352)    0           ['concatenate_26[0][0]',         \n",
      "                                                                  'conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 8, 8, 352)   1408        ['concatenate_27[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 8, 8, 352)    0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 12)     38016       ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 8, 8, 364)    0           ['concatenate_27[0][0]',         \n",
      "                                                                  'conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 8, 8, 364)   1456        ['concatenate_28[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 8, 8, 364)    0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 12)     39312       ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_29 (Concatenate)   (None, 8, 8, 376)    0           ['concatenate_28[0][0]',         \n",
      "                                                                  'conv2d_31[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 8, 8, 376)   1504        ['concatenate_29[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 8, 8, 376)    0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 12)     40608       ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 8, 8, 388)    0           ['concatenate_29[0][0]',         \n",
      "                                                                  'conv2d_32[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 8, 8, 388)   1552        ['concatenate_30[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 8, 8, 388)    0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 12)     41904       ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 8, 8, 400)    0           ['concatenate_30[0][0]',         \n",
      "                                                                  'conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 8, 8, 400)   1600        ['concatenate_31[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 8, 8, 400)    0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 12)     43200       ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 8, 8, 412)    0           ['concatenate_31[0][0]',         \n",
      "                                                                  'conv2d_34[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 8, 8, 412)   1648        ['concatenate_32[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 8, 8, 412)    0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 12)     44496       ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_33 (Concatenate)   (None, 8, 8, 424)    0           ['concatenate_32[0][0]',         \n",
      "                                                                  'conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 8, 8, 424)   1696        ['concatenate_33[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 8, 8, 424)    0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 12)     45792       ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_34 (Concatenate)   (None, 8, 8, 436)    0           ['concatenate_33[0][0]',         \n",
      "                                                                  'conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 8, 8, 436)   1744        ['concatenate_34[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 8, 8, 436)    0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 12)     47088       ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_35 (Concatenate)   (None, 8, 8, 448)    0           ['concatenate_34[0][0]',         \n",
      "                                                                  'conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 8, 8, 448)   1792        ['concatenate_35[0][0]']         \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 448)    0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 448)         0           ['activation_38[0][0]']          \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           4490        ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 10)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,037,818\n",
      "Trainable params: 1,019,722\n",
      "Non-trainable params: 18,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defining the dataset\n",
    "if FLAGS.dataset_name == \"CIFAR-10\":\n",
    "    dataset = CIFAR10Dataset()\n",
    "\n",
    "\n",
    "# 1. Load a dataset.\n",
    "print(\"\\n===Loading %s data...\" % FLAGS.dataset_name)\n",
    "\n",
    "if FLAGS.dataset_name == 'ImageNet':\n",
    "    if FLAGS.model_name == 'inceptionv3':\n",
    "        img_size = 299\n",
    "    else:\n",
    "        img_size = 224\n",
    "    X_test_all, Y_test_all = dataset.get_test_data(img_size, 0, 200)\n",
    "else:\n",
    "    X_test_all, Y_test_all = dataset.get_test_dataset()\n",
    "\n",
    "# 2. Load a trained model.\n",
    "\n",
    "sess1 = load_tf_session()\n",
    "keras.backend.set_learning_phase(0)\n",
    "# Define input TF placeholder\n",
    "\n",
    "x = tf.compat.v1.placeholder(tf.float32, shape=(None, dataset.image_size, dataset.image_size, dataset.num_channels))\n",
    "y = tf.compat.v1.placeholder(tf.float32, shape=(None, dataset.num_classes))\n",
    "# x = tf.placeholder(tf.float32, shape=(None, dataset.image_size, dataset.image_size, dataset.num_channels))\n",
    "# y = tf.placeholder(tf.float32, shape=(None, dataset.num_classes))\n",
    "\n",
    " \n",
    "with tf.compat.v1.variable_scope(FLAGS.model_name):\n",
    "# with tf.variable_scope(FLAGS.model_name):\n",
    "    \"\"\"\n",
    "    Create a model instance for prediction.\n",
    "    The scaling argument, 'input_range_type': {1: [0,1], 2:[-0.5, 0.5], 3:[-1, 1]...}\n",
    "    \"\"\"\n",
    "    model = dataset.load_model_by_name(FLAGS.model_name, logits=False, input_range_type=1)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['acc'])\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the pre-trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on raw legitimate examples 0.9483\n",
      "Mean confidence on ground truth classes 0.9214\n"
     ]
    }
   ],
   "source": [
    "# 3. Evaluate the trained model.\n",
    "print (\"Evaluating the pre-trained model...\")\n",
    "Y_pred_all = model.predict(X_test_all)\n",
    "mean_conf_all = calculate_mean_confidence(Y_pred_all, Y_test_all)\n",
    "accuracy_all = calculate_accuracy(Y_pred_all, Y_test_all)\n",
    "print('Test accuracy on raw legitimate examples %.4f' % (accuracy_all))\n",
    "print('Mean confidence on ground truth classes %.4f' % (mean_conf_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 examples.\n",
      "Selected index in test set (sorted): 0-52:1,54-57:1,59,60,63,65-67:1,69,70,72-77:1,79,81-87:1,90,91-97:3,98,101,103-106:1,110,113-115:1,117,118,121,122,128,130,159\n",
      "Test accuracy on selected legitimate examples 1.0000\n",
      "Mean confidence on ground truth classes, selected 0.9554\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Select some examples to attack.\n",
    "import hashlib\n",
    "from datasets_utils import get_first_n_examples_id_each_class\n",
    "\n",
    "if FLAGS.select:\n",
    "    # Filter out the misclassified examples.\n",
    "    correct_idx = get_correct_prediction_idx(Y_pred_all, Y_test_all)\n",
    "    if FLAGS.test_mode:\n",
    "        # Only select the first example of each class.\n",
    "        correct_and_selected_idx = get_first_n_examples_id_each_class(Y_test_all[correct_idx])\n",
    "        selected_idx = [ correct_idx[i] for i in correct_and_selected_idx ]\n",
    "    else:\n",
    "        if not FLAGS.balance_sampling:\n",
    "            selected_idx = correct_idx[:FLAGS.nb_examples]\n",
    "        else:\n",
    "            # select the same number of examples for each class label.\n",
    "            nb_examples_per_class = int(FLAGS.nb_examples / Y_test_all.shape[1])\n",
    "            correct_and_selected_idx = get_first_n_examples_id_each_class(Y_test_all[correct_idx], n=nb_examples_per_class)\n",
    "            selected_idx = [ correct_idx[i] for i in correct_and_selected_idx ]\n",
    "else:\n",
    "    selected_idx = np.array(range(FLAGS.nb_examples))\n",
    "\n",
    "from utils.output import format_number_range\n",
    "selected_example_idx_ranges = format_number_range(sorted(selected_idx))\n",
    "print ( \"Selected %d examples.\" % len(selected_idx))\n",
    "print ( \"Selected index in test set (sorted): %s\" % selected_example_idx_ranges )\n",
    "X_test, Y_test, Y_pred = X_test_all[selected_idx], Y_test_all[selected_idx], Y_pred_all[selected_idx]\n",
    "\n",
    "# The accuracy should be 100%.\n",
    "accuracy_selected = calculate_accuracy(Y_pred, Y_test)\n",
    "mean_conf_selected = calculate_mean_confidence(Y_pred, Y_test)\n",
    "print('Test accuracy on selected legitimate examples %.4f' % (accuracy_selected))\n",
    "print('Mean confidence on ground truth classes, selected %.4f\\n' % (mean_conf_selected))\n",
    "\n",
    "task = {}\n",
    "task['dataset_name'] = FLAGS.dataset_name\n",
    "task['model_name'] = FLAGS.model_name\n",
    "task['accuracy_test'] = accuracy_all\n",
    "task['mean_confidence_test'] = mean_conf_all\n",
    "\n",
    "task['test_set_selected_length'] = len(selected_idx)\n",
    "task['test_set_selected_idx_ranges'] = selected_example_idx_ranges\n",
    "task['test_set_selected_idx_hash'] = hashlib.sha1(str(selected_idx).encode('utf-8')).hexdigest()\n",
    "task['accuracy_test_selected'] = accuracy_selected\n",
    "task['mean_confidence_test_selected'] = mean_conf_selected\n",
    "\n",
    "task_id = \"%s_%d_%s_%s\" % \\\n",
    "        (task['dataset_name'], task['test_set_selected_length'], task['test_set_selected_idx_hash'][:5], task['model_name'])\n",
    "\n",
    "FLAGS.result_folder = os.path.join(FLAGS.result_folder, task_id)\n",
    "if not os.path.isdir(FLAGS.result_folder):\n",
    "    print(\"Making the Folder %s\" % FLAGS.result_folder)\n",
    "    os.makedirs(FLAGS.result_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# 5. Generate adversarial examples.\n",
    "from attacks import maybe_generate_adv_examples\n",
    "from utils.squeeze import reduce_precision_py\n",
    "from utils.parameter_parser import parse_params\n",
    "attack_string_hash = hashlib.sha1(FLAGS.attacks.encode('utf-8')).hexdigest()[:5]\n",
    "sample_string_hash = task['test_set_selected_idx_hash'][:5]\n",
    "\n",
    "from datasets_utils import get_next_class, get_least_likely_class\n",
    "Y_test_target_next = get_next_class(Y_test)\n",
    "Y_test_target_ll = get_least_likely_class(Y_pred)\n",
    "\n",
    "X_test_adv_list = []\n",
    "X_test_adv_discretized_list = []\n",
    "Y_test_adv_discretized_pred_list = []\n",
    "\n",
    "attack_string_list = filter(lambda x:len(x)>0, FLAGS.attacks.lower().split(';'))\n",
    "to_csv = []\n",
    "\n",
    "X_adv_cache_folder = os.path.join(FLAGS.result_folder, 'adv_examples')\n",
    "adv_log_folder = os.path.join(FLAGS.result_folder, 'adv_logs')\n",
    "predictions_folder = os.path.join(FLAGS.result_folder, 'predictions')\n",
    "for folder in [X_adv_cache_folder, adv_log_folder, predictions_folder]:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "predictions_fpath = os.path.join(predictions_folder, \"legitimate.npy\")\n",
    "np.save(predictions_fpath, Y_pred, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running attack: fgsm {'eps': 0.0156}\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_fgsm_eps=0.0156.pickle].\n",
      "\n",
      "---Attack (uint8): fgsm?eps=0.0156\n",
      "Success rate: 86.00%, Mean confidence of SAEs: 96.93%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 0.8644, Li dist: 0.0157, L0 dist_value: 99.4%, L0 dist_pixel: 99.8%\n",
      "\n",
      "Running attack: bim {'eps': 0.008, 'eps_iter': 0.0012}\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_bim_eps=0.008&eps_iter=0.0012.pickle].\n",
      "\n",
      "---Attack (uint8): bim?eps=0.008&eps_iter=0.0012\n",
      "Success rate: 92.00%, Mean confidence of SAEs: 98.75%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 0.3670, Li dist: 0.0078, L0 dist_value: 92.1%, L0 dist_pixel: 99.4%\n",
      "\n",
      "Running attack: carlinili {'targeted': 'next', 'confidence': 5.0}\n",
      "targeted value: next\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_carlinili_targeted=next&confidence=5.pickle].\n",
      "\n",
      "---Attack (uint8): carlinili?targeted=next&confidence=5\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 98.28%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 0.4431, Li dist: 0.0122, L0 dist_value: 88.5%, L0 dist_pixel: 98.8%\n",
      "\n",
      "Running attack: carlinili {'targeted': 'll', 'confidence': 5.0}\n",
      "targeted value: ll\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_carlinili_targeted=ll&confidence=5.pickle].\n",
      "\n",
      "---Attack (uint8): carlinili?targeted=ll&confidence=5\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 97.38%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 0.5211, Li dist: 0.0141, L0 dist_value: 90.8%, L0 dist_pixel: 99.4%\n",
      "\n",
      "Running attack: deepfool {'overshoot': 10.0}\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_deepfool_overshoot=10.pickle].\n",
      "\n",
      "---Attack (uint8): deepfool?overshoot=10\n",
      "Success rate: 97.00%, Mean confidence of SAEs: 83.79%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 0.2350, Li dist: 0.0279, L0 dist_value: 98.9%, L0 dist_pixel: 99.3%\n",
      "\n",
      "Running attack: carlinil2 {'targeted': 'next', 'batch_size': 100, 'max_iterations': 1000, 'confidence': 5.0}\n",
      "targeted value: next\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_carlinil2_targeted=next&batch_size=100&max_iterations=1000&confidence=5.pickle].\n",
      "\n",
      "---Attack (uint8): carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=5\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 97.90%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 0.2843, Li dist: 0.0334, L0 dist_value: 51.0%, L0 dist_pixel: 75.3%\n",
      "\n",
      "Running attack: carlinil2 {'targeted': 'll', 'batch_size': 100, 'max_iterations': 1000, 'confidence': 5.0}\n",
      "targeted value: ll\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_carlinil2_targeted=ll&batch_size=100&max_iterations=1000&confidence=5.pickle].\n",
      "\n",
      "---Attack (uint8): carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=5\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 97.21%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 0.3524, Li dist: 0.0401, L0 dist_value: 60.4%, L0 dist_pixel: 84.7%\n",
      "\n",
      "Running attack: carlinil0 {'targeted': 'next', 'confidence': 5.0}\n",
      "targeted value: next\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_carlinil0_targeted=next&confidence=5.pickle].\n",
      "\n",
      "---Attack (uint8): carlinil0?targeted=next&confidence=5\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 98.21%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 2.0409, Li dist: 0.6401, L0 dist_value: 1.8%, L0 dist_pixel: 1.8%\n",
      "\n",
      "Running attack: carlinil0 {'targeted': 'll', 'confidence': 5.0}\n",
      "targeted value: ll\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_carlinil0_targeted=ll&confidence=5.pickle].\n",
      "\n",
      "---Attack (uint8): carlinil0?targeted=ll&confidence=5\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 97.60%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 2.4788, Li dist: 0.6926, L0 dist_value: 2.4%, L0 dist_pixel: 2.4%\n",
      "\n",
      "Running attack: jsma {'targeted': 'next'}\n",
      "targeted value: next\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_jsma_targeted=next.pickle].\n",
      "\n",
      "---Attack (uint8): jsma?targeted=next\n",
      "Success rate: 100.00%, Mean confidence of SAEs: 42.61%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 4.9920, Li dist: 0.8968, L0 dist_value: 2.9%, L0 dist_pixel: 7.8%\n",
      "\n",
      "Running attack: jsma {'targeted': 'll'}\n",
      "targeted value: ll\n",
      "Loading adversarial examples from [CIFAR-10_100_4fc98_densenet_jsma_targeted=ll.pickle].\n",
      "\n",
      "---Attack (uint8): jsma?targeted=ll\n",
      "Success rate: 98.00%, Mean confidence of SAEs: 39.18%\n",
      "### Statistics of the SAEs:\n",
      "L2 dist: 5.5989, Li dist: 0.8958, L0 dist_value: 3.7%, L0 dist_pixel: 9.8%\n"
     ]
    }
   ],
   "source": [
    "if FLAGS.clip >= 0:\n",
    "    epsilon = FLAGS.clip\n",
    "    print (\"Clip the adversarial perturbations by +-%f\" % epsilon)\n",
    "    max_clip = np.clip(X_test + epsilon, 0, 1)\n",
    "    min_clip = np.clip(X_test - epsilon, 0, 1)\n",
    "\n",
    "for attack_string in attack_string_list:\n",
    "    attack_log_fpath = os.path.join(adv_log_folder, \"%s_%s.log\" % (task_id, attack_string.replace(\"?\",\"_\")))\n",
    "    attack_name, attack_params = parse_params(attack_string)\n",
    "    print ( \"\\nRunning attack: %s %s\" % (attack_name, attack_params))\n",
    "\n",
    "    if 'targeted' in attack_params:\n",
    "        targeted = attack_params['targeted']\n",
    "        print (\"targeted value: %s\" % targeted)\n",
    "        if targeted == 'next':\n",
    "            Y_test_target = Y_test_target_next\n",
    "        elif targeted == 'll':\n",
    "            Y_test_target = Y_test_target_ll\n",
    "        elif targeted == False:\n",
    "            attack_params['targeted'] = False\n",
    "            Y_test_target = Y_test.copy()\n",
    "    else:\n",
    "        targeted = False\n",
    "        attack_params['targeted'] = False\n",
    "        Y_test_target = Y_test.copy()\n",
    "\n",
    "    x_adv_fname = \"%s_%s.pickle\" % (task_id, attack_string.replace(\"?\",\"_\"))\n",
    "    x_adv_fpath = os.path.join(X_adv_cache_folder, x_adv_fname)\n",
    "\n",
    "    X_test_adv, aux_info = maybe_generate_adv_examples(sess1, model, x, y, X_test, Y_test_target, attack_name, attack_params,\n",
    "                                                        use_cache = x_adv_fpath, verbose=FLAGS.verbose, attack_log_fpath=attack_log_fpath)\n",
    "\n",
    "    if FLAGS.clip > 0:\n",
    "        # This is L-inf clipping.\n",
    "        X_test_adv = np.clip(X_test_adv, min_clip, max_clip)\n",
    "\n",
    "    X_test_adv_list.append(X_test_adv)\n",
    "\n",
    "    if isinstance(aux_info, float):\n",
    "        duration = aux_info\n",
    "    else:\n",
    "        duration = aux_info['duration']\n",
    "\n",
    "    dur_per_sample = duration / len(X_test_adv)\n",
    "\n",
    "    # 5.0 Output predictions.\n",
    "    Y_test_adv_pred = model.predict(X_test_adv)\n",
    "    predictions_fpath = os.path.join(predictions_folder, \"%s.npy\"% attack_string.replace(\"?\",\"_\"))\n",
    "    np.save(predictions_fpath, Y_test_adv_pred, allow_pickle=False)\n",
    "\n",
    "    # 5.1 Evaluate the adversarial examples being discretized to uint8.\n",
    "    print (\"\\n---Attack (uint8): %s\" % attack_string)\n",
    "    # All data should be discretized to uint8.\n",
    "    X_test_adv_discret = reduce_precision_py(X_test_adv, 256)\n",
    "    X_test_adv_discretized_list.append(X_test_adv_discret)\n",
    "    Y_test_adv_discret_pred = model.predict(X_test_adv_discret)\n",
    "    Y_test_adv_discretized_pred_list.append(Y_test_adv_discret_pred)\n",
    "\n",
    "    rec = evaluate_adversarial_examples(X_test, Y_test, X_test_adv_discret, Y_test_target.copy(), targeted, Y_test_adv_discret_pred)\n",
    "    rec['dataset_name'] = FLAGS.dataset_name\n",
    "    rec['model_name'] = FLAGS.model_name\n",
    "    rec['attack_string'] = attack_string.replace(\"?\",\"_\")\n",
    "    rec['duration_per_sample'] = dur_per_sample\n",
    "    rec['discretization'] = True\n",
    "    to_csv.append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from feature_squeezing import FeatureSqueezingDetector\n",
    "from magnet_mnist import MagNetDetector as MagNetDetectorMNIST\n",
    "from magnet_cifar import MagNetDetector as MagNetDetectorCIFAR\n",
    "\n",
    "from tensorflow.python.platform import flags\n",
    "FLAGS = flags.FLAGS\n",
    "from utils.output import write_to_csv\n",
    "# from Segmentation_detector_Yolo import SegmentationDetector \n",
    "\n",
    "def get_tpr_fpr(true_labels, pred_labels):\n",
    "    TP = np.sum(np.logical_and(pred_labels == 1, true_labels == 1))\n",
    "    FP = np.sum(np.logical_and(pred_labels == 1, true_labels == 0))\n",
    "\n",
    "    AP = np.sum(true_labels)\n",
    "    AN = np.sum(1-true_labels)\n",
    "\n",
    "    tpr = TP/AP if AP>0 else np.nan\n",
    "    fpr = FP/AN if AN>0 else np.nan\n",
    "\n",
    "    return tpr, fpr, TP, AP\n",
    "\n",
    "\n",
    "def evalulate_detection_test(Y_detect_test, Y_detect_pred):\n",
    "    accuracy = sklearn.metrics.accuracy_score(Y_detect_test, Y_detect_pred, normalize=True, sample_weight=None)\n",
    "    tpr, fpr, tp, ap = get_tpr_fpr(Y_detect_test, Y_detect_pred)\n",
    "    return accuracy, tpr, fpr, tp, ap\n",
    "\n",
    "\n",
    "from tinydb import TinyDB, Query\n",
    "\n",
    "class DetectionEvaluator:\n",
    "    \"\"\"\n",
    "    Get a dataset;\n",
    "        Failed adv as benign / Failed adv as adversarial.\n",
    "    For each detector:\n",
    "        Train\n",
    "        Test\n",
    "        Report performance\n",
    "            Detection rate on each attack.\n",
    "            Detection on SAEs / FAEs.\n",
    "            ROC-AUC.\n",
    "\n",
    "    A detector should have this simplified interface:\n",
    "        Y_pred = detector(X)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, result_folder, csv_fname, dataset_name):\n",
    "        pass\n",
    "        # set_base_model()\n",
    "        self.model = model\n",
    "        self.task_dir = result_folder\n",
    "        self.csv_fpath = os.path.join(result_folder, csv_fname)\n",
    "        print(self.csv_fpath)\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        if not os.path.isdir(self.task_dir):\n",
    "            os.makedirs(self.task_dir)\n",
    "\n",
    "    def get_attack_id(self, attack_name):\n",
    "        return self.attack_name_id[attack_name]\n",
    "\n",
    "    def build_detection_dataset(self, X, Y_label, Y_pred, selected_idx, X_adv_list, Y_adv_pred_list,\n",
    "                                 attack_names, attack_string_hash, clip, Y_test_target_next, Y_test_target_ll):\n",
    "        # X_train, Y_train, X_test, Y_test, test_idx, failed_adv_idx = \\\n",
    "        #     get_detection_train_test_set(X, Y_label, X_adv_list, Y_adv_pred_list, attack_names)\n",
    "\n",
    "        \"\"\"\n",
    "        Data Model:\n",
    "            index, attack_id, misclassified, train\n",
    "            14,     0,           0,             1\n",
    "        \"\"\"\n",
    "\n",
    "        self.attack_names = attack_names.lower().split(';')\n",
    "        self.attack_name_id = {}\n",
    "        self.attack_name_id['legitimate'] = 0\n",
    "        # attack_names = filter(lambda x:len(x)>0, attack_names.lower().split(';'))\n",
    "        attack_names = attack_names.lower().split(';')\n",
    "        for i,attack_name in enumerate(attack_names):\n",
    "            # print(\"AttackNames\",i,attack_name)\n",
    "            self.attack_name_id[attack_name] = i+1\n",
    "            # print(self.attack_name_id[attack_name])\n",
    "\n",
    "        X_adv_all = np.concatenate(X_adv_list)\n",
    "        X_leg_all = X[:len(X_adv_all)]\n",
    "        # print(len(X_leg_all),\"kkkkkkk\")\n",
    "        self.X_detect = X_detect = np.concatenate([X_leg_all, X_adv_all])\n",
    "        # TODO: this could be wrong in non-default data selection mode.\n",
    "        Y_label_adv = Y_label[selected_idx]\n",
    "\n",
    "        detection_db_path = os.path.join(self.task_dir, \"detection_db_%s_clip_%s.json\" % (attack_string_hash, clip))\n",
    "\n",
    "        if os.path.isfile(detection_db_path):\n",
    "            self.db = TinyDB(detection_db_path)\n",
    "            self.query = Query()\n",
    "            print (\"Loaded an existing detection dataset.\")\n",
    "            return\n",
    "        else:\n",
    "            print (\"Preparing the detection dataset...\")\n",
    "\n",
    "        # 1. Split Train and Test \n",
    "        random.seed(1234)\n",
    "        length = len(X_detect)\n",
    "        train_ratio = 0.5\n",
    "        train_idx = random.sample(range(length), int(train_ratio*length))\n",
    "        train_test_seq = [1 if idx in train_idx else 0 for idx in range(length) ]\n",
    "\n",
    "        # 2. Tag the misclassified examples, both legitimate and adversarial.\n",
    "        # TODO: Differentiate the successful examples between targeted and non-targeted.\n",
    "        misclassified_seq = list(np.argmax(Y_label[:len(X_leg_all)], axis=1) != np.argmax(Y_pred[:len(X_leg_all)], axis=1))\n",
    "        for Y_adv_pred in Y_adv_pred_list:\n",
    "            misclassified_seq_adv = list(np.argmax(Y_adv_pred, axis=1) != np.argmax(Y_label_adv, axis=1))\n",
    "            misclassified_seq += misclassified_seq_adv\n",
    "\n",
    "        success_adv_seq = [False] * len(X_leg_all)\n",
    "        for ( Y_adv_pred),(attack_name_i) in zip(Y_adv_pred_list,attack_names):\n",
    "            attack_name = attack_name_i\n",
    "            if 'targeted=ll' in attack_name:\n",
    "                # print(\"Yes LL\")\n",
    "                success_adv_seq_attack = list(np.argmax(Y_adv_pred, axis=1) == np.argmax(Y_test_target_ll, axis=1))\n",
    "            elif 'targeted=next' in attack_name:\n",
    "                # print(\"Yes Next\")\n",
    "                success_adv_seq_attack = list(np.argmax(Y_adv_pred, axis=1) == np.argmax(Y_test_target_next, axis=1))\n",
    "            else:\n",
    "                # The same as misclassified.\n",
    "                success_adv_seq_attack = list(np.argmax(Y_adv_pred, axis=1) != np.argmax(Y_label_adv, axis=1))\n",
    "            success_adv_seq += success_adv_seq_attack\n",
    "\n",
    "\n",
    "\n",
    "        # 3. Tag the attack ID, 0 as legitimate.\n",
    "        attack_id_seq = [0]*len(X_leg_all)\n",
    "        # attack_id_seq = [0]*len(X_detect)\n",
    "        # print(\"Hello_Hi\")\n",
    "        # print(len(attack_names),len(attack_id_seq))\n",
    "        for i,attack_name in enumerate(attack_names):\n",
    "            attack_id_seq += [i+1]*len(X_adv_list[0])\n",
    "            # print(\"Hello_friend\")\n",
    "        # print(len(X_detect) , len(train_test_seq) , len(misclassified_seq) , len(attack_id_seq))\n",
    "        assert len(X_detect) == len(train_test_seq) == len(misclassified_seq) == len(attack_id_seq)\n",
    "        # assert len(X_detect) == len(train_test_seq) == len(misclassified_seq)\n",
    "\n",
    "        self.db = TinyDB(detection_db_path)\n",
    "        self.query = Query()\n",
    "        # print(len(attack_id_seq),len(X_detect))\n",
    "        for i in range(len(X_detect)):\n",
    "            attack_id = attack_id_seq[i]\n",
    "            misclassified = 1 if misclassified_seq[i] == True else 0\n",
    "            success = 1 if success_adv_seq[i] == True else 0\n",
    "            train = train_test_seq[i]\n",
    "            rec = {'index': i, 'attack_id': attack_id, 'misclassified': misclassified, 'success': success, 'train': train}\n",
    "            self.db.insert(rec)\n",
    "\n",
    "    def get_data_from_db_records(self, recs):\n",
    "        if len(recs) == 0:\n",
    "            return None, None\n",
    "        X_idx = [rec['index'] for rec in recs]\n",
    "        X = self.X_detect[np.array(X_idx)]\n",
    "        Y = np.array([1 if rec['attack_id']>0 else 0 for rec in recs])\n",
    "        return X, Y\n",
    "\n",
    "    def get_training_testing_data(self, train = True):\n",
    "        db = self.db\n",
    "        query = self.query\n",
    "\n",
    "        recs = db.search(query.train == 1)\n",
    "        X_train, Y_train = self.get_data_from_db_records(recs)\n",
    "\n",
    "        recs = db.search(query.train == 0)\n",
    "        X_test, Y_test = self.get_data_from_db_records(recs)\n",
    "\n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "    def get_adversarial_data(self, only_testing, success, attack_name=None, include_legitimate=False):\n",
    "        db = self.db\n",
    "        query = self.query\n",
    "\n",
    "        conditions_and = []\n",
    "        if only_testing:\n",
    "            conditions_and.append(query.train == 0)\n",
    "\n",
    "        if attack_name is None:\n",
    "            conditions_and.append(query.attack_id > 0)\n",
    "            # print(query.attack_id,\"1\")\n",
    "        else:\n",
    "            # print(attack_name,\"2\")\n",
    "            attack_id = self.get_attack_id(attack_name)\n",
    "            conditions_and.append(query.attack_id == attack_id)\n",
    "\n",
    "        if success:\n",
    "            conditions_and.append(query.success == 1)\n",
    "        else:\n",
    "            conditions_and.append(query.success == 0)\n",
    "\n",
    "        conditions = reduce(lambda a,b:a&b, conditions_and)\n",
    "        print (\"conditions: %s \" % conditions)\n",
    "\n",
    "        recs = db.search(conditions)\n",
    "\n",
    "        if include_legitimate:\n",
    "            if only_testing:\n",
    "                conditions = (query.attack_id == 0) & (query.train == 0)\n",
    "            else:\n",
    "                conditions = query.attack_id == 0\n",
    "            # print (\"additional conditions: %s \" % conditions)\n",
    "            recs += db.search(conditions)\n",
    "\n",
    "        return self.get_data_from_db_records(recs)\n",
    "\n",
    "    def get_sae_testing_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=True, success=True, attack_name=attack_name)\n",
    "\n",
    "    def get_sae_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=False, success=True, attack_name=attack_name)\n",
    "\n",
    "    def get_fae_testing_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=True, success=False, attack_name=attack_name)\n",
    "\n",
    "    def get_fae_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=False, success=False, attack_name=attack_name)\n",
    "\n",
    "    def get_all_non_fae_testing_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=True, success=True, attack_name=attack_name, include_legitimate=True)\n",
    "\n",
    "    def get_all_non_fae_data(self, attack_name=None):\n",
    "        return self.get_adversarial_data(only_testing=False, success=True, attack_name=attack_name, include_legitimate=True)\n",
    "\n",
    "    def get_detector_by_name(self, detector_name):\n",
    "        model = self.model\n",
    "        detector = None\n",
    "\n",
    "        if detector_name.startswith('FeatureSqueezing'):\n",
    "            detector = FeatureSqueezingDetector(model, detector_name)\n",
    "        elif detector_name.startswith('Segmentation'):\n",
    "            detector = SegmentationDetector()\n",
    "        elif detector_name.startswith('MagNet'):\n",
    "            if self.dataset_name == 'MNIST':\n",
    "                detector = MagNetDetectorMNIST(model, detector_name)\n",
    "            elif self.dataset_name == \"CIFAR-10\":\n",
    "                detector = MagNetDetectorCIFAR(model, detector_name)\n",
    "\n",
    "        return detector\n",
    "\n",
    "    def evaluate_detections(self, params_str,X_test_all,Y_test_all):\n",
    "        X_train, Y_train, X_test, Y_test = self.get_training_testing_data()\n",
    "        # print(Y_test)\n",
    "        # Example: --detection \"FeatureSqueezing?distance_measure=l1&squeezers=median_smoothing_2,bit_depth_4;\"\n",
    "        detector_names = [ele.strip() for ele in params_str.split(';') if ele.strip()!= '']\n",
    "        dataset_name = self.dataset_name\n",
    "        csv_fpath = \"./segmentation_detection_%s_saes.csv\" % dataset_name \n",
    "        fieldnames = ['detector', 'threshold', 'fpr','fpr_all_test_clean','Accuracy_all_test_clean'] + list(self.attack_names) + ['overall']\n",
    "        print(fieldnames)\n",
    "        to_csv = []\n",
    "\n",
    "        for detector_name in detector_names:\n",
    "            detector = self.get_detector_by_name(detector_name)\n",
    "            if detector is None:\n",
    "                print (\"Skipped an unknown detector [%s]\" % detector_name.split('?')[0])\n",
    "                continue\n",
    "            detector.train(X_train, Y_train)\n",
    "            Y_test_pred, Y_test_pred_score = detector.test(X_test)\n",
    "            # print( \"Hello Boy\" )\n",
    "            accuracy, tpr, fpr, tp, ap = evalulate_detection_test(Y_test, Y_test_pred)\n",
    "            fprs, tprs, thresholds = roc_curve(Y_test, Y_test_pred_score)\n",
    "            roc_auc = auc(fprs, tprs)\n",
    "            print( \"\\n\" )\n",
    "            print (\"Detector: %s\" % detector_name)\n",
    "            print( \"\\n\" )\n",
    "            print (\"Accuracy: %f\\tTPR: %f\\tFPR: %f\\tROC-AUC: %f\" % (accuracy, tpr, fpr, roc_auc))\n",
    "\n",
    "            Y_test_pred_t, Y_test_pred_score_t = detector.test(X_test_all)\n",
    "            # print(Y_test_pred_t[0], \"Hello Boy\" )\n",
    "            Y_test_thresh = np.zeros(np.array(np.argmax(Y_test_all,axis=1)).shape) \n",
    "            accuracy_all, tpr_all, fpr_all, tp_all, ap_all = evalulate_detection_test(Y_test_thresh, Y_test_pred_t)\n",
    "            print(\"Accuracy_all_test:%.3f\" % accuracy_all,\" FPR_all_test:%.3f\" % fpr_all,\"For all TestSet\")\n",
    "            \n",
    "            rec = {}\n",
    "            rec['detector'] = detector_name\n",
    "            if hasattr(detector, 'threshold'):\n",
    "                rec['threshold'] = detector.threshold\n",
    "            else:\n",
    "                rec['threshold'] = None\n",
    "            rec['fpr'] = fpr\n",
    "            rec['Accuracy_all_test_clean'] = accuracy_all\n",
    "            rec['fpr_all_test_clean'] = fpr_all\n",
    "            overall_detection_rate_saes = 0\n",
    "            nb_saes = 0\n",
    "            for attack_name in self.attack_names:\n",
    "                print( \"\\n\" )\n",
    "                # No adversarial examples for training for the current detection methods.\n",
    "                # X_sae, Y_sae = self.get_sae_testing_data(attack_name)\n",
    "                if FLAGS.detection_train_test_mode:\n",
    "                    print(attack_name)\n",
    "                    X_sae, Y_sae = self.get_sae_testing_data(attack_name)\n",
    "                else:\n",
    "                    X_sae, Y_sae = self.get_sae_data(attack_name)\n",
    "                Y_test_pred, Y_test_pred_score = detector.test(X_sae)\n",
    "                # print(Y_sae,Y_test_pred,\"Hala Madrid\")\n",
    "                _, tpr, _, tp, ap = evalulate_detection_test(Y_sae, Y_test_pred)\n",
    "                print (\"Detection rate on SAEs: %.4f \\t %3d/%3d \\t %s\" % (tpr, tp, ap, attack_name))\n",
    "                overall_detection_rate_saes += tpr * len(Y_sae)\n",
    "                nb_saes += len(Y_sae)\n",
    "                rec[attack_name] = tpr\n",
    "                # print (\"overall_detection_rate_saes/nb_saes: %d/%d\" % (overall_detection_rate_saes, nb_saes))\n",
    "            # print(len(nb_saes))\n",
    "            print(nb_saes)\n",
    "            print (\"Overall detection rate on SAEs: %f (%d/%d)\" % (overall_detection_rate_saes/nb_saes, overall_detection_rate_saes, nb_saes))\n",
    "            rec['overall'] = float(overall_detection_rate_saes/nb_saes)\n",
    "            to_csv.append(rec)\n",
    "\n",
    "            # No adversarial examples for training for the current detection methods.\n",
    "            # X_sae_all, Y_sae_all = self.get_sae_testing_data()\n",
    "            print (\"### Excluding FAEs:\")\n",
    "            if FLAGS.detection_train_test_mode:\n",
    "                X_nfae_all, Y_nfae_all = self.get_all_non_fae_testing_data()\n",
    "            else:\n",
    "                X_nfae_all, Y_nfae_all = self.get_all_non_fae_data()\n",
    "            Y_pred, Y_pred_score = detector.test(X_nfae_all)\n",
    "            _, tpr, _, tp, ap = evalulate_detection_test(Y_nfae_all, Y_pred)\n",
    "            fprs, tprs, thresholds = roc_curve(Y_nfae_all, Y_pred_score)\n",
    "\n",
    "            # print (\"threshold\\tfpr\\ttpr\")\n",
    "            # for i, threshold  in enumerate(thresholds):\n",
    "            #     print (\"%.4f\\t%.4f\\t%.4f\" % (threshold, fprs[i], tprs[i]))\n",
    "\n",
    "            roc_auc = auc(fprs, tprs)\n",
    "            print (\"Overall TPR: %f\\tROC-AUC: %f\" % (tpr, roc_auc))\n",
    "\n",
    "            # FAEs\n",
    "            if FLAGS.detection_train_test_mode:\n",
    "                X_fae, Y_fae = self.get_fae_testing_data()\n",
    "            else:\n",
    "                X_fae, Y_fae = self.get_fae_data()\n",
    "            Y_test_pred, Y_test_pred_score = detector.test(X_fae)\n",
    "            _, tpr, _, tp, ap = evalulate_detection_test(Y_fae, Y_test_pred)\n",
    "            print (\"Overall detection rate on FAEs: %.4f \\t %3d/%3d\" % (tpr, tp, ap))\n",
    "        # print(csv_fpath,fieldnames)\n",
    "        write_to_csv(to_csv, csv_fpath, fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if FLAGS.detection != '':\n",
    "#     # from detections.base import DetectionEvaluator\n",
    "\n",
    "#     result_folder_detection = os.path.join(FLAGS.result_folder, \"detection\")\n",
    "#     csv_fname = \"%s_attacks_%s_detection.csv\" % (task_id, attack_string_hash)\n",
    "#     de = DetectionEvaluator(model, result_folder_detection, csv_fname, FLAGS.dataset_name)\n",
    "#     Y_test_all_pred = model.predict(X_test_all)\n",
    "#     attack_string_list = filter(lambda x:len(x)>0, FLAGS.attacks.lower().split(';'))\n",
    "#     # de.build_detection_dataset(X_test_all, Y_test_all, Y_test_all_pred, selected_idx, X_test_adv_discretized_list, Y_test_adv_discretized_pred_list, attack_string_list, attack_string_hash, FLAGS.clip, Y_test_target_next, Y_test_target_ll)\n",
    "#     de.build_detection_dataset(X_test_all, Y_test_all, Y_test_all_pred, selected_idx, X_test_adv_discretized_list, Y_test_adv_discretized_pred_list,\n",
    "#                                 FLAGS.attacks, attack_string_hash, FLAGS.clip, Y_test_target_next, Y_test_target_ll)\n",
    "#     de.evaluate_detections(FLAGS.detection,X_test_all,Y_test_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCSVM-Siamese Detection method Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TensorFlow session and set Keras backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\keras\\backend.py:451: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. Load a trained model.\n",
    "\n",
    "sess2 = load_tf_session()\n",
    "keras.backend.set_learning_phase(0)\n",
    "\n",
    "# graph_0 = tf.compat.v1.get_default_graph()\n",
    "# with sess2.as_default():\n",
    "    # sess2 = tf.compat.v1.Session()\n",
    "with sess2.as_default():\n",
    "    with open('C:/Users/ahmad/Desktop/JupyterFilesAUB/CIFAR_10/Siamese/Sia_Dense_Conv_checkpoints/Dense_Net_CIFAR_10_Conv_156_spd.json', 'r') as arch_file:\n",
    "        CIFAR_10_Dense_model = keras.models.model_from_json(arch_file.read(), custom_objects={'tf': tf,'keras': keras})\n",
    "    arch_file.close()\n",
    "    CIFAR_10_Dense_model.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/CIFAR_10/Siamese/Sia_Dense_Conv_checkpoints/Dense_Net_CIFAR_10_Conv_156.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "def identity_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "def triplet_loss(x, alpha = 1):\n",
    "# def triplet_loss(x, alpha = 10):\n",
    "# def triplet_loss(x, alpha = 30):\n",
    "    # Triplet Loss function.\n",
    "    anchor,positive,negative = x\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    "    return loss\n",
    "\n",
    "anchor_input   = layers.Input((32, 32, 3),name=\"anchor\"  )\n",
    "positive_input = layers.Input((32, 32, 3),name=\"positive\")    \n",
    "negative_input = layers.Input((32, 32, 3),name=\"negative\") \n",
    "# embedding_network = CIFAR_10_Dense_model   \n",
    "\n",
    "# tower_1 = embedding_network(anchor_input)\n",
    "normal_layer_1 = tf.keras.layers.BatchNormalization(name=\"bat_1\")(CIFAR_10_Dense_model.output)\n",
    "normal_layer_1 = layers.Dense(512, activation=\"relu\")(normal_layer_1)\n",
    "normal_layer_1 = tf.keras.layers.BatchNormalization(name=\"bat_2\")(normal_layer_1)\n",
    "normal_layer_1 = layers.Dense(256, activation=\"relu\")(normal_layer_1)\n",
    "# normal_layer_1 = layers.Dense(64, activation=\"relu\")(normal_layer_1)\n",
    "\n",
    "embedding = Model(CIFAR_10_Dense_model.input, normal_layer_1, name=\"Embedding\")\n",
    "\n",
    "A = embedding(anchor_input  )\n",
    "P = embedding(positive_input)\n",
    "N = embedding(negative_input)\n",
    "\n",
    "loss = layers.Lambda(triplet_loss)([A, P, N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TensorFlow session and set Keras backend.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " anchor (InputLayer)            [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " positive (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " negative (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " Embedding (Functional)         (None, 256)          1398384     ['anchor[0][0]',                 \n",
      "                                                                  'positive[0][0]',               \n",
      "                                                                  'negative[0][0]']               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None,)              0           ['Embedding[0][0]',              \n",
      "                                                                  'Embedding[1][0]',              \n",
      "                                                                  'Embedding[2][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,398,384\n",
      "Trainable params: 1,378,368\n",
      "Non-trainable params: 20,016\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lrt   = 1e-4\n",
    "optimizer = Adam(learning_rate=lrt)\n",
    "# 2. Load a trained model.\n",
    "sess3 = load_tf_session()\n",
    "keras.backend.set_learning_phase(0)\n",
    "\n",
    "# graph_1 = tf.compat.v1.get_default_graph()\n",
    "# with sess3.as_default():\n",
    "    # sess3 = tf.compat.v1.Session()\n",
    "with sess3.as_default():\n",
    "    siamese_network = Model(\n",
    "        inputs=[anchor_input, positive_input, negative_input], outputs=loss)\n",
    "\n",
    "    siamese_network.load_weights(\"C:/Users/ahmad/Desktop/JupyterFilesAUB/CIFAR_10_Dataset/Yolo/CiFAR_10_Hard_Set/ep_alpha1_01_los0.238_CIFAR_10_Triplets_MedClahe.h5\")\n",
    "    siamese_network.compile(loss=identity_loss, optimizer=optimizer)\n",
    "siamese_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_Net_CIFAR_10 = keras.models.load_model(\"All_CIFAR_10_U_Net_DC_10Ep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median-Filtering_Clahe...\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "train_images = medfilter(train_images)\n",
    "# test_images  = medfilter(test_images )\n",
    "\n",
    "train_images = train_images.astype(np.float32)/255\n",
    "# test_images  = test_images .astype(np.float32)/255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'one_svm_model_CIFAR_10_DC_GRAY.sav'\n",
    "OC_SVM_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "def test(X_test,Y_test,threshold, flag):\n",
    "    dim = 64\n",
    "    adv_cls = 10\n",
    "    x_adv_SVM = np.zeros(len(Y_test))\n",
    "    X_test_ = []\n",
    "    for img in X_test:\n",
    "        image = cv2.resize(img, (dim,dim)) #64*64*1 Images\n",
    "        img_uint8 = np.clip(np.rint(image * 255), 0, 255).astype(np.uint8)\n",
    "        image = cv2.cvtColor(img_uint8,cv2.COLOR_RGB2GRAY)#Image now is 255-level\n",
    "        image = np.expand_dims(image, axis=2)\n",
    "        X_test_.append(image)\n",
    "    X_test_ = np.array(X_test_)\n",
    "    X_test_U = X_test_/255.\n",
    "    print(X_test_U.shape)\n",
    "    X_test_mask           = U_Net_CIFAR_10.predict(X_test_U,verbose=0)\n",
    "    X_test_preds_mask_t   = (X_test_mask > 0.5).astype(np.uint8)\n",
    "    X_test_masks          = np.array(X_test_preds_mask_t)\n",
    "    adv_labels_SVM = x_adv_SVM\n",
    "    X_adv_OCsvm = X_test_masks.reshape((len(X_test_masks), -1))\n",
    "    result = OC_SVM_model.predict(X_adv_OCsvm )\n",
    "    oc_indices = np.where(result==-1)[0]\n",
    "    adv_labels_SVM[oc_indices] = adv_cls\n",
    "#############################################################\n",
    "    Y_test  = np.array(Y_test )\n",
    "    X_test_orig       = []\n",
    "    for img in X_test:\n",
    "        img_uint8 = np.clip(np.rint(img * 255), 0, 255).astype(np.uint8)\n",
    "        X_test_orig.append(img_uint8)\n",
    "    # X_test_orig = np.array(medfilter(X_test_orig))/255\n",
    "    X_test_orig = medfilter(X_test_orig )\n",
    "    X_test_orig = np.array(X_test_orig)\n",
    "    \n",
    "    # X_test_orig  = X_test_orig.astype(np.float32)/255\n",
    "    # print(X_test_orig.shape)\n",
    "    num_classes_clean   = max(train_labels) + 1\n",
    "    digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n",
    "    siamese_labels_1    = []\n",
    "    for idx1 in range(len(X_test_orig)):\n",
    "        # add a matching example\n",
    "        x1     = X_test_orig [idx1]\n",
    "        label1 = Y_test      [idx1]\n",
    "        x1     = x1/255.\n",
    "        idx2   = random.choice(digit_indices_clean[label1])\n",
    "        x2     = train_images[idx2]\n",
    "        idx3   = random.choice(digit_indices_clean[label1])\n",
    "        x3     = train_images[idx3]\n",
    "        anchor   = embedding.predict(np.expand_dims(x1, axis=0),verbose = 0)\n",
    "        positive = embedding.predict(np.expand_dims(x2, axis=0),verbose = 0)\n",
    "        positive3 = embedding.predict(np.expand_dims(x3, axis=0),verbose = 0)#Two_Sample\n",
    "\n",
    "        anchor   = np.reshape(anchor  ,256)\n",
    "        positive = np.reshape(positive,256)\n",
    "        positive3 = np.reshape(positive3,256)#Two_Sample\n",
    "        pos_dist = np.dot(anchor,positive)/(norm(anchor)*norm(positive))\n",
    "        pos_dist3 = np.dot(anchor,positive3)/(norm(anchor)*norm(positive3))#Two_Sample\n",
    "\n",
    "        pos_dist_final = min(pos_dist,pos_dist3)#Two_Sample\n",
    "        siamese_labels_1.append(pos_dist_final)\n",
    "\n",
    "    print(len(siamese_labels_1))\n",
    "    print(siamese_labels_1[0])\n",
    "    if len(siamese_labels_1) !=0:\n",
    "        x_ = np.zeros(len(siamese_labels_1))\n",
    "        x_[np.where(np.array(siamese_labels_1)>threshold)[0]] = 1\n",
    "        res = len(np.where(x_==0)[0])/len(siamese_labels_1)\n",
    "        print(\"Siamese_Detection\",res,\"%\")      \n",
    "        x_[oc_indices] = 0     #SVM Detection\n",
    "        return x_== 0 # inverse logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Segmentation\n",
      "['detector', 'threshold', 'fpr', 'fpr_all_test_clean', 'Accuracy_all_test_clean', 'fgsm?eps=0.0156', 'bim?eps=0.008&eps_iter=0.0012', 'carlinili?targeted=next&confidence=5', 'carlinili?targeted=ll&confidence=5', 'deepfool?overshoot=10', 'carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=5', 'carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=5', 'carlinil0?targeted=next&confidence=5', 'carlinil0?targeted=ll&confidence=5', 'jsma?targeted=next', 'jsma?targeted=ll', 'overall']\n",
      "(100, 64, 64, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n",
      "c:\\ProgramData\\anaconda3\\envs\\tensorenv\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.88989264\n",
      "Siamese_Detection 0.08 %\n",
      "Detector: Siamese_Triplet_Hard_Set_Two_Sample_Med_Clahe_SVM_Alpha_1\n",
      "Accuracy: 0.910000\tTPR: nan\tFPR: 0.090000\t\n",
      "(10000, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "0.8465636\n",
      "Siamese_Detection 0.0951 %\n",
      "Accuracy_all_test:0.899  FPR_all_test:0.101 For all TestSet\n",
      "(86, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "0.885967\n",
      "Siamese_Detection 0.6744186046511628 %\n",
      "Detection rate on SAEs: 0.6744 \t  58/ 86 \t fgsm?eps=0.0156\n",
      "overall_detection_rate_saes/nb_saes: 58/86\n",
      "(92, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "0.121245824\n",
      "Siamese_Detection 0.7717391304347826 %\n",
      "Detection rate on SAEs: 0.7826 \t  72/ 92 \t bim?eps=0.008&eps_iter=0.0012\n",
      "overall_detection_rate_saes/nb_saes: 130/178\n",
      "Yes Next\n",
      "(100, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.46548885\n",
      "Siamese_Detection 0.95 %\n",
      "Detection rate on SAEs: 0.9500 \t  95/100 \t carlinili?targeted=next&confidence=5\n",
      "overall_detection_rate_saes/nb_saes: 225/278\n",
      "Yes LL\n",
      "(100, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.26661527\n",
      "Siamese_Detection 1.0 %\n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t carlinili?targeted=ll&confidence=5\n",
      "overall_detection_rate_saes/nb_saes: 325/378\n",
      "(97, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "0.25573337\n",
      "Siamese_Detection 0.9072164948453608 %\n",
      "Detection rate on SAEs: 0.9072 \t  88/ 97 \t deepfool?overshoot=10\n",
      "overall_detection_rate_saes/nb_saes: 413/475\n",
      "Yes Next\n",
      "(100, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.42837095\n",
      "Siamese_Detection 0.95 %\n",
      "Detection rate on SAEs: 0.9500 \t  95/100 \t carlinil2?targeted=next&batch_size=100&max_iterations=1000&confidence=5\n",
      "overall_detection_rate_saes/nb_saes: 508/575\n",
      "Yes LL\n",
      "(100, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.23962098\n",
      "Siamese_Detection 1.0 %\n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t carlinil2?targeted=ll&batch_size=100&max_iterations=1000&confidence=5\n",
      "overall_detection_rate_saes/nb_saes: 608/675\n",
      "Yes Next\n",
      "(100, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.6506943\n",
      "Siamese_Detection 0.92 %\n",
      "Detection rate on SAEs: 0.9200 \t  92/100 \t carlinil0?targeted=next&confidence=5\n",
      "overall_detection_rate_saes/nb_saes: 700/775\n",
      "Yes LL\n",
      "(100, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.17461345\n",
      "Siamese_Detection 1.0 %\n",
      "Detection rate on SAEs: 1.0000 \t 100/100 \t carlinil0?targeted=ll&confidence=5\n",
      "overall_detection_rate_saes/nb_saes: 800/875\n",
      "Yes Next\n",
      "(100, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.46644986\n",
      "Siamese_Detection 0.97 %\n",
      "Detection rate on SAEs: 0.9700 \t  97/100 \t jsma?targeted=next\n",
      "overall_detection_rate_saes/nb_saes: 897/975\n",
      "Yes LL\n",
      "(98, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "0.12687917\n",
      "Siamese_Detection 0.9897959183673469 %\n",
      "Detection rate on SAEs: 0.9898 \t  97/ 98 \t jsma?targeted=ll\n",
      "overall_detection_rate_saes/nb_saes: 994/1073\n",
      "1073\n",
      "Overall detection rate on SAEs: 0.926375 (994/1073)\n",
      "(27, 64, 64, 1)\n",
      "Median-Filtering_Clahe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Temp\\ipykernel_396\\1065988495.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  digit_indices_clean = [np.where(np.array(train_labels) == i)[0] for i in range(int(num_classes_clean))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "0.69411445\n",
      "Siamese_Detection 0.037037037037037035 %\n",
      "Overall detection rate on FAEs: 0.0370 \t   1/ 27\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello Siamese\")\n",
    "# detector_names = [ele.strip() for ele in params_str.split(';') if ele.strip()!= '']\n",
    "attack_names = FLAGS.attacks.lower().split(';')\n",
    "dataset_name = \"CIFAR_10\"\n",
    "# detector_name = \"Siamese_Triplet_Hard_Set_Med_Clahe_SVM_Alpha_1\"\n",
    "# csv_fpath = \"./Siamese_Triplets_Hard_Set_Med_Clahe_SVM_Alpha_1_detection_%s_saes_point5_thresh.csv\" % dataset_name \n",
    "detector_name = \"Siamese_Triplet_Hard_Set_Two_Sample_Med_Clahe_SVM_Alpha_1\"\n",
    "csv_fpath = \"./Siamese_Triplets_Hard_Set_Two_Sample_Med_Clahe_SVM_Alpha_1_detection_%s_saes_point5_thresh.csv\" % dataset_name \n",
    "fieldnames = ['detector', 'threshold', 'fpr','fpr_all_test_clean','Accuracy_all_test_clean'] + list(attack_names) + ['overall']\n",
    "print(fieldnames)\n",
    "to_csv = []\n",
    "threshold = 0.5\n",
    "flag = 0\n",
    "\n",
    "Y_test_pred  = test(X_test_all[:len(selected_idx)],np.argmax(Y_test_all[:len(selected_idx)],axis=1),threshold, flag)\n",
    "X_test_zeros = np.zeros(np.array(Y_test_pred).shape) \n",
    "accuracy, tpr, fpr, tp, ap = evalulate_detection_test(X_test_zeros, Y_test_pred)\n",
    "# fprs, tprs, thresholds = roc_curve(Y_test, Y_test_pred_score)\n",
    "# roc_auc = auc(fprs, tprs)\n",
    "\n",
    "print (\"Detector: %s\" % detector_name)\n",
    "print (\"Accuracy: %f\\tTPR: %f\\tFPR: %f\\t\" % (accuracy, tpr, fpr))\n",
    "\n",
    "Y_test_pred_t = test(X_test_all,np.argmax(Y_test_all,axis=1),threshold, flag)\n",
    "# print(Y_test_pred_t[0], \"Hello Boy\" )\n",
    "Y_test_thresh = np.zeros(np.array(np.argmax(Y_test_all,axis=1)).shape) \n",
    "accuracy_all, tpr_all, fpr_all, tp_all, ap_all = evalulate_detection_test(Y_test_thresh, Y_test_pred_t)\n",
    "print(\"Accuracy_all_test:%.3f\" % accuracy_all,\" FPR_all_test:%.3f\" % fpr_all,\"For all TestSet\")\n",
    "flag = 1\n",
    "rec  = {}\n",
    "rec['detector'] = detector_name\n",
    "rec['threshold'] = threshold\n",
    "rec['fpr'] = fpr\n",
    "rec['Accuracy_all_test_clean'] = accuracy_all\n",
    "rec['fpr_all_test_clean'] = fpr_all\n",
    "overall_detection_rate_saes = 0\n",
    "nb_saes = 0\n",
    "\n",
    "count = 0\n",
    "y_check_non = []\n",
    "x_check_non = []\n",
    "for attack_name in attack_names:\n",
    "    if 'targeted=ll' in attack_name:\n",
    "        print(\"Yes LL\")\n",
    "        Y_adv_orig  = np.argmax(Y_test_adv_discretized_pred_list[count],axis=1)\n",
    "        adv_indeces = np.where (Y_adv_orig==np.argmax(Y_test_target_ll, axis=1))[0]\n",
    "        y_check     = Y_adv_orig[adv_indeces]\n",
    "        Y_test_pred       = test(np.array(X_test_adv_discretized_list[count])[adv_indeces],\n",
    "                                     y_check,threshold, flag)\n",
    "        non_adv_indeces = np.where (Y_adv_orig!=np.argmax(Y_test_target_ll, axis=1))[0]\n",
    "        if len(non_adv_indeces)>0:\n",
    "            y_check_non.append(Y_adv_orig[non_adv_indeces])\n",
    "            x_check_non.append(np.array(X_test_adv_discretized_list[count])[non_adv_indeces])\n",
    "        count+=1\n",
    "    elif 'targeted=next' in attack_name:\n",
    "        print(\"Yes Next\")\n",
    "        Y_adv_orig  = np.argmax(Y_test_adv_discretized_pred_list[count],axis=1)\n",
    "        adv_indeces = np.where (Y_adv_orig==np.argmax(Y_test_target_next, axis=1))[0]\n",
    "        y_check     = Y_adv_orig[adv_indeces]\n",
    "        Y_test_pred       = test(np.array(X_test_adv_discretized_list[count])[adv_indeces],\n",
    "                                   y_check ,threshold, flag)\n",
    "        non_adv_indeces = np.where (Y_adv_orig!=np.argmax(Y_test_target_next, axis=1))[0]\n",
    "        if len(non_adv_indeces)>0:\n",
    "            y_check_non.append(Y_adv_orig[non_adv_indeces])\n",
    "            x_check_non.append(np.array(X_test_adv_discretized_list[count])[non_adv_indeces])\n",
    "        count+=1\n",
    "    else:\n",
    "        Y_adv_orig  = np.argmax(Y_test_adv_discretized_pred_list[count],axis=1)\n",
    "        adv_indeces = np.where (Y_adv_orig!=np.argmax(Y_test_all[selected_idx], axis=1))[0]\n",
    "        y_check     = Y_adv_orig[adv_indeces]\n",
    "        Y_test_pred = test(np.array(X_test_adv_discretized_list[count])[adv_indeces],\n",
    "                                    y_check,threshold, flag)\n",
    "        non_adv_indeces = np.where (Y_adv_orig==np.argmax(Y_test_all[selected_idx], axis=1))[0]\n",
    "        if len(non_adv_indeces)>0:\n",
    "            y_check_non.append(Y_adv_orig[non_adv_indeces])\n",
    "            x_check_non.append(np.array(X_test_adv_discretized_list[count])[non_adv_indeces])\n",
    "        count+=1\n",
    "    Y_sae             = np.ones(np.array(Y_test_pred).shape) \n",
    "    _, tpr, _, tp, ap = evalulate_detection_test(Y_sae, Y_test_pred)\n",
    "    print (\"Detection rate on SAEs: %.4f \\t %3d/%3d \\t %s\" % (tpr, tp, ap, attack_name))\n",
    "    overall_detection_rate_saes += tpr * len(Y_sae)\n",
    "    nb_saes += len(Y_sae)\n",
    "    rec[attack_name] = tpr\n",
    "    print (\"overall_detection_rate_saes/nb_saes: %d/%d\" % (overall_detection_rate_saes, nb_saes))\n",
    "print(nb_saes)\n",
    "print (\"Overall detection rate on SAEs: %f (%d/%d)\" % (overall_detection_rate_saes/nb_saes, overall_detection_rate_saes, nb_saes))\n",
    "rec['overall'] = float(overall_detection_rate_saes/nb_saes)\n",
    "to_csv.append(rec)\n",
    "y_check_non = np.array(list(itertools.chain(*y_check_non)))\n",
    "x_check_non = np.array(list(itertools.chain(*x_check_non)))\n",
    "#FAEs\n",
    "Y_test_pred = test(x_check_non,\n",
    "                                   y_check_non ,threshold, flag)\n",
    "Y_fae       = np.ones(np.array(y_check_non).shape) \n",
    "_, tpr, _, tp, ap = evalulate_detection_test(Y_fae, Y_test_pred)\n",
    "print (\"Overall detection rate on FAEs: %.4f \\t %3d/%3d\" % (tpr, tp, ap))\n",
    "write_to_csv(to_csv, csv_fpath, fieldnames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
